{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction_To_Natural_Language_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPt+ll0pn5Q/9qrzNoClr3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya5614/Kaggle/blob/main/Introduction_To_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9ssRMhsEeIj"
      },
      "source": [
        "# 自然言語処理入門"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC_YDGfyEeLW"
      },
      "source": [
        "# [1] 自然言語処理の基本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjoS0xkwEeNl"
      },
      "source": [
        "## 1.文字列の操作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSddCcM6l97P"
      },
      "source": [
        "### 1-1.スライス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKlGgoSImNz-"
      },
      "source": [
        "target = 'naturallanguage'\n",
        "ans  = target[5::2]\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMBxhgbzm_YM"
      },
      "source": [
        "### 1-2.文字列を逆順に並べ換える"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6gnHkrrnS9n"
      },
      "source": [
        "target = 'naturallanguage'\n",
        "ans  = target[-5::-2]\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsXdwXxKnUA3"
      },
      "source": [
        "### 1-3.文字列の結合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlEG91YXnvyF"
      },
      "source": [
        "target = ('a', 'b', 'c', 'd', 'e')\n",
        "ans = 'X'.join(target)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXhhLb17EeQC"
      },
      "source": [
        "## 2.自然言語の前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iAQB7QLn6fB"
      },
      "source": [
        "### 2-1.正規表現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IheW-ZNqoC2s"
      },
      "source": [
        "**Pythonで正規表現を使う**\n",
        "```\n",
        "re.findall(r'正規表現', 検索文字列)\n",
        "result = re.findall(r'\\d', target)\n",
        "# \\Dは0~9の数字以外を表し\n",
        "result = re.findall(r'\\D', target)\n",
        "# [abc]は[]のなかのいずれかの一文字、つまりa, b, cを含む箇所の抽出を表し、大文字小文字も区別\n",
        "result = re.findall(r'[abc]', target)\n",
        "# -を用いて範囲を指定することも可能\n",
        "result = re.findall(r'[a-c]', target)\n",
        "# [の直後に^をつけ、文字の補集合を表すことが出来る。[^a-c]は小文字のa,b,c以外を表す\n",
        "result = re.findall(r'[^a-c]', target)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJN7x2zdpRIY"
      },
      "source": [
        "### 2-2.数字・記号の置き換え"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS2JRRGkpiac"
      },
      "source": [
        "**文字列を置換する**\n",
        "```\n",
        "re.sub(正規表現, 置換先文字列, 対象文字列)\n",
        "```\n",
        "**アルファベット以外は正規表現[^a-zA-Z]で表現する**\n",
        "```\n",
        "text = \"I bought 3 apples @ the grocery store:)\"\n",
        "preprocessed_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "preprocessed_text\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlasO2EEp-pV"
      },
      "source": [
        "import re\n",
        "text = \"abc123\"\n",
        "preprocessed_text = re.sub(r'[^a-zA-Z]', '5', text)\n",
        "print(preprocessed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbv30K6DqG_C"
      },
      "source": [
        "### 2-3.URLの削除"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_1nq9ULqpYc"
      },
      "source": [
        "# モジュールのインポート\n",
        "import re\n",
        "\n",
        "# 文字列の定義\n",
        "text = \"click this URL: https://aaa.com/\"\n",
        "\n",
        "# 変数textからURLを削除\n",
        "preprocessed_text = re.sub('https?://\\S+', '', text)\n",
        "\n",
        "# 結果の表示\n",
        "print(preprocessed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyk0eTcPqs5E"
      },
      "source": [
        "### 2-4.正規化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1wLAz_Zq1hx"
      },
      "source": [
        "**全ての大文字を小文字に変換する**\n",
        "```\n",
        "テキスト.lower()\n",
        "```\n",
        "**日本語文字列を半角・全角変換する**\n",
        "```\n",
        "import mojimoji\n",
        "# 全角から半角への変換\n",
        "mojimoji.zen_to_han(u'ネコ')\n",
        "\n",
        "# 半角から全角への変換\n",
        "mojimoji.han_to_zen(u'ﾈｺ')\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMj2zXLGrUBv"
      },
      "source": [
        "# 英語を大文字から小文字に変換\n",
        "text_en = \"NLP\"\n",
        "preprocessed_text_en = text_en.lower()\n",
        "print(preprocessed_text_en)\n",
        "\n",
        "# 日本語を全角から半角に変換\n",
        "import mojimoji\n",
        "text_jp ='シゼンゲンゴ'\n",
        "preprocessed_text_jp = mojimoji.zen_to_han(text_jp)\n",
        "print(preprocessed_text_jp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptu7pmL0EeSU"
      },
      "source": [
        "## 3.形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQv4X6Sqrg0B"
      },
      "source": [
        "### 3-1.英文のトークン化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sunMxcf4rqol"
      },
      "source": [
        "**文章を意味の区切りの最小単位（単語）に分割する(トークン化)**\n",
        "```\n",
        "# スペースで分割\n",
        "text = \"I bought some apples.\"\n",
        "preprocessed_text = text.split()\n",
        "preprocessed_text\n",
        "```\n",
        "**ピリオドをスペース+ピリオドに置き換えたのち、split()で単語に分割する**\n",
        "```\n",
        "# replace(置換元文字列、置換先文字列)\n",
        "\n",
        "text = \"I bought some apples.\"\n",
        "preprocessed_text = text.replace('.', ' .').split()\n",
        "preprocessed_text\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21EP9DjpsNRh"
      },
      "source": [
        "text = \"I am studying NLP\"\n",
        "preprocessed_text = text.split()\n",
        "print(preprocessed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m191CgyhsUn7"
      },
      "source": [
        "### 3-2.日本語のトークン化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOao6aNgshsX"
      },
      "source": [
        "**日本語のトークン化ツール**\n",
        "```\n",
        "import MeCab\n",
        "m = MeCab.Tagger(\"-Owakati\")\n",
        "```\n",
        "**テキストの分かち書きをする**\n",
        "```\n",
        "# パーサー.parse(テキスト)\n",
        "print(m.parse(\"日本語の形態素解析は難しい。\"))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPFEFuoNs7ii"
      },
      "source": [
        "# MeCabのインポート\n",
        "import MeCab\n",
        "# パーサーの設定\n",
        "m = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "text = \"私は毎日コーヒーを飲みます。\"\n",
        "# 分かち書き\n",
        "ans = m.parse(text)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYV_PqwltBub"
      },
      "source": [
        "### 3-3.日本語の形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x9q-6DCtQwY"
      },
      "source": [
        "**単語ごとの品詞を確認する**\n",
        "```\n",
        "# 出力は単語の文字列 読み 原形 品詞の種類 活用の種類 活用形が順に表示される\n",
        "import MeCab\n",
        "m = MeCab.Tagger(\"-Ochasen\")\n",
        "m.parse(\"日本語は難しい。\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_z2Ld2MtdN6"
      },
      "source": [
        "# MeCabのインポート\n",
        "import MeCab\n",
        "# パーサーの設定\n",
        "m = MeCab.Tagger(\"-Ochasen\")\n",
        "\n",
        "text = \"私は毎日コーヒーを飲みます。\"\n",
        "# 形態素解析\n",
        "ans = m.parse(text)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RukAc29xEeZs"
      },
      "source": [
        "## 4.構文解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq4rMve7tm73"
      },
      "source": [
        "### 4-1.係り受け解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Yaz2o4tqtT"
      },
      "source": [
        "**各単語の係り先について決定する作業(係り受け解析)**\n",
        "```\n",
        "import CaboCha\n",
        "c = CaboCha.Parser()\n",
        "```\n",
        "**係り受け解析の結果をツリー形式で表示する**\n",
        "```\n",
        "# パーサー.parseToString(テキスト)\n",
        "\n",
        "tree = c.parseToString(text)\n",
        "print(tree)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrtTE8PfuM-_"
      },
      "source": [
        "# CaboChaのインポート\n",
        "import CaboCha\n",
        "# パーサーの設定\n",
        "c = CaboCha.Parser()\n",
        "\n",
        "text = \"新型コロナウイルスの感染拡大が世界的に大きな影響を与える中で、地域の現状について認識を完全に共有しました。\"\n",
        "# 係り受け解析を行い、ツリー形式で表示\n",
        "tree = c.parseToString(text)\n",
        "print(tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrmcH5oKuQ2e"
      },
      "source": [
        "### 4-2.詳細な係り受け解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04YjFmBHuV2v"
      },
      "source": [
        "**係り受け解析結果を、lattice形式と呼ばれるより詳細な形態にて表示する**\n",
        "```\n",
        "# ツリー.toString(CaboCha.FORMAT_LATTICE)\n",
        "\n",
        "import CaboCha\n",
        "c = CaboCha.Parser()\n",
        "text = \"太郎はこの本を二郎を見た女性に渡した。\"\n",
        "tree =  c.parse(text)\n",
        "\n",
        "# lattice形式に変換して表示\n",
        "print(tree.toString(CaboCha.FORMAT_LATTICE))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdWYVEbfujXz"
      },
      "source": [
        "# CaboChaのインポート\n",
        "import CaboCha\n",
        "# パーサーの設定\n",
        "c = CaboCha.Parser()\n",
        "\n",
        "text = \"新型コロナウイルスの感染拡大が世界的に大きな影響を与える中で、地域の現状について認識を完全に共有しました。\"\n",
        "\n",
        "# 係り受け解析を行い、lattice形式で表示\n",
        "tree = c.parse(text)\n",
        "ans = tree.toString(CaboCha.FORMAT_LATTICE)\n",
        "print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y6wXEJEEeei"
      },
      "source": [
        "# [2] 自然言語処理の手法（機械学習）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCU4u6aE7OB"
      },
      "source": [
        "## 1.n-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7s5I8FRu0p-"
      },
      "source": [
        "### 1-1.n-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-g-jHvnu9m9"
      },
      "source": [
        "```\n",
        "文章を連続するN個の文字、もしくはN個の単語単位で単語を切り出す手法\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHsH8DGDE7Qu"
      },
      "source": [
        "## 2.Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMqQIq-kvAxz"
      },
      "source": [
        "### 2-1.Bag of Words(BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l59-BCSuvP9p"
      },
      "source": [
        "```\n",
        "文章を含まれる単語とその頻度により表現する手法\n",
        "```\n",
        "**文章をベクトル化する**\n",
        "```\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# ベクトル化する文章\n",
        "corpus = [\n",
        "     'This is the first document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?',\n",
        " ]\n",
        "```\n",
        "**corpus内部の単語の出現回数をカウントする**\n",
        "```\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_vectorizer.fit(corpus)\n",
        "```\n",
        "**出現単語とidを辞書型で表示する**\n",
        "```\n",
        "print(bow_vectorizer.vocabulary_)\n",
        "```\n",
        "**corpusをベクトルに変換する**\n",
        "```\n",
        "# 出力は疎行列ですが、X.toarray()でnumpy.ndarrayに変換する\n",
        "X = bow_vectorizer.transform(corpus)\n",
        "print(X.toarray())\n",
        "```\n",
        "**出現回数の順に並んだ行列の列名を確認する**\n",
        "```\n",
        "print(bow_vectorizer.get_feature_names())\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0HLAx03wgIL"
      },
      "source": [
        "# CountVectorizerのインポート\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "     'I go to work by bus.',\n",
        "     'The bus is always crowded.',\n",
        "     'You will stay by my side.',\n",
        "]\n",
        "\n",
        "# CountVectorizerのインスタンス化\n",
        "bow_vectorizer = CountVectorizer()\n",
        "# fit\n",
        "bow_vectorizer.fit(corpus)\n",
        "print(\"corpus内の単語:\", bow_vectorizer.vocabulary_)\n",
        "\n",
        "# テキストの変換\n",
        "X = bow_vectorizer.transform(corpus)\n",
        "print(\"出力ベクトル\", X.toarray())\n",
        "print(\"出力ベクトルの形状\", X.shape)\n",
        "print(\"出力ベクトルの列名:\", bow_vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3PVYEPmE7TF"
      },
      "source": [
        "## 3.tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNcLxeXSAnUS"
      },
      "source": [
        "### 3-1.tf-idfのアルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxDvXggXAwf9"
      },
      "source": [
        "**tf**\n",
        "```\n",
        "*   単語の文章内の出現頻度をカウント\n",
        "```\n",
        "**idf**\n",
        "```\n",
        "*   ある単語が出てくる文書頻度の逆数\n",
        "*   多くの文書に出てくる単語は重要度が低いことを意味する\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0CvLO7BLym"
      },
      "source": [
        "### 3-2.tf-idfの使用方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyHf7MHEBRax"
      },
      "source": [
        "**tf-idfの計算をする**\n",
        "```\n",
        "# モジュールのimport、文書A, Bの定義\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documentA = 'the man went out for a walk'\n",
        "documentB = 'the children sat around the fire'\n",
        "\n",
        "# 文書A, Bの変換をする\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "type(vectors)\n",
        "```\n",
        "**疎行列を密行列に変換する**\n",
        "```\n",
        "dense = vectors.todense()\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "```\n",
        "**見やすくするためにpandasのデータフレーム型にする**\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb0mHXLgCAZV"
      },
      "source": [
        "# TfidfVectorizerのインポート\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# 文書の定義\n",
        "documentA = 'the man went out for a walk'\n",
        "documentB = 'the children sat around the fire'\n",
        "\n",
        "# TfidfVectorizerのインスタンス化し、vectorizerと命名\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# documentA,Bのfit、変換を行い、変数vectorsに代入\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "\n",
        "# vectorizer内の単語リストを取得\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# 取得した単語リストの表示\n",
        "print(feature_names)\n",
        "\n",
        "# 変数vectorsを密行列に変換\n",
        "dense = vectors.todense()\n",
        "\n",
        "#　リストに変換し、データフレーム型に変換\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "\n",
        "# データフレームの表示\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An27vPFXE7Vi"
      },
      "source": [
        "## 4.LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kKME8ajCWAS"
      },
      "source": [
        "### 4-1.LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y55n-LAaCSC8"
      },
      "source": [
        "**トピックモデル**\n",
        "```\n",
        "文書がどのようなトピック（分野など）から、どのような割合で構成されているかについて推定するアルゴリズムであり、教師なし分類モデル\n",
        "```\n",
        "**LDA(Latent Dirichlet Allocation)**\n",
        "```\n",
        "*   トピック数kを仮定する\n",
        "*   各単語にトピックを割り当てることで、これらのk個のトピックを文書mに分散させる (この分散をαとする）\n",
        "*   文書mの各単語wについて、そのトピックは間違っているが、他のすべての単語には正しいトピックが割り当てられていると仮定する\n",
        "*   文書mにどのようなトピックがあるか、すべての文書で単語wが特定のトピックに割り当てられた回数 (この分布をβとする）に基づき、確率的に単語をトピックに割り当てる\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXxDzIUVDAdM"
      },
      "source": [
        "### 4-2.LDAの使用方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyBbsVWYDFjr"
      },
      "source": [
        "**単語とその単語の整数で表されたidのマッピングを作成するモジュールを読み込む**\n",
        "```\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "```\n",
        "**解析対象の文書を用意する**\n",
        "```\n",
        "documents = [\"I quickly put on my red winter jacket, black snow pants, waterproof boots, homemade mittens, and handknit scarf\",\n",
        "              \"The incessant ticking and chiming echoed off the weathered walls of the clock repair shop\",\n",
        "              \"Nervously, I unfolded the wrinkled and stained letter from my long-dead ancestor\",\n",
        "              \"Into the suitcase, I carelessly threw a pair of ripped jeans, my favorite sweater from high school, an old pair of tube socks with stripes, and $20,000 in cash\",\n",
        "              \"The mangy, scrawny stray dog hurriedly gobbled down the grain-free, organic dog food\",\n",
        "              \"The truth is that, apart from breaking news, the modern media will not report anything much after 5 pm\",\n",
        "              \"In terms of the number of people involved, it was a major disaster, and some of us followed the regular daily news bulletins\",\n",
        "              \"The first reports only a few weeks after this devastating earthquake reveal that tourist bookings have dropped drastically\",\n",
        "              \"Low-lying Louisiana stands to suffer some inundation if climate change projections come true\"]\n",
        "```\n",
        "**各文書を大文字から小文字に変換することで正規化を行い、スペースで単語に分割する**\n",
        "```\n",
        "# ストップワードの指定\n",
        "stop_words = ['a','the', 'and', 'this', 'that',  'have', 'of']\n",
        "# 正規化、トークン化、stopwordsの削除\n",
        "texts = [[word for word in document.lower().split() if word not in stop_words] for document in documents]\n",
        "#一つ目の文章の前処理結果を確認\n",
        "print(texts[0])\n",
        "```\n",
        "**単語id・出現頻度の情報を持つ配列に変換する**\n",
        "```\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "print(corpus[0])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdvGONjfDzT1"
      },
      "source": [
        "**LDAによる解析を行う**\n",
        "```\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=3, id2word=dictionary)\n",
        "# 1つ目のトピックを確認\n",
        "print(lda.show_topics()[0])\n",
        "\n",
        "# 新たな文書のトピックを解析する\n",
        "test_documents = [\"Millions of years ago, changes in the earth's climate caused animal and plant life to diversify.\"]\n",
        "\n",
        "# 前処理を行い、作成した辞書を適用する\n",
        "# 前処理\n",
        "test_texts = [[word for word in document.lower().split()] for document in test_documents]\n",
        "\n",
        "# 作成した辞書を適用\n",
        "test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
        "\n",
        "# LDAの適用結果を見る\n",
        "for topics in lda[test_corpus]:\n",
        "    print(topics)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouvYAGGwESgc"
      },
      "source": [
        "# gensimのインポート\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# 文書の定義\n",
        "documents = [\"I quickly put on my red winter jacket, black snow pants, waterproof boots, homemade mittens, and handknit scarf\",\n",
        "              \"The incessant ticking and chiming echoed off the weathered walls of the clock repair shop\",\n",
        "              \"Nervously, I unfolded the wrinkled and stained letter from my long-dead ancestor\",\n",
        "              \"Into the suitcase, I carelessly threw a pair of ripped jeans, my favorite sweater from high school, an old pair of tube socks with stripes, and $20,000 in cash\",\n",
        "              \"The mangy, scrawny stray dog hurriedly gobbled down the grain-free, organic dog food\",\n",
        "              \"The truth is that, apart from breaking news, the modern media will not report anything much after 5 pm\",\n",
        "              \"In terms of the number of people involved, it was a major disaster, and some of us followed the regular daily news bulletins\",\n",
        "              \"The first reports only a few weeks after this devastating earthquake reveal that tourist bookings have dropped drastically\",\n",
        "              \"Low-lying Louisiana stands to suffer some inundation if climate change projections come true\"]\n",
        "\n",
        "# ストップワードの指定\n",
        "stop_words = ['a','the', 'and', 'this', 'that',  'have', 'of']\n",
        "\n",
        "# 正規化、トークン化\n",
        "texts = [[word for word in document.lower().split() if word not in stop_words] for document in documents]\n",
        "\n",
        "# textsを単語id, 単語, 単語出現回数の情報を持つ辞書型データに変換\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# textsを単語id・出現頻度の情報を持つ配列\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# モデルの設定\n",
        "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=3, id2word=dictionary)\n",
        "# 1番目の文章のトピックを表示\n",
        "print(\"1番目の文章のトピック:\", lda.show_topics()[0])\n",
        "\n",
        "# 新たな文書のトピックを判定\n",
        "test_documents = [\"Millions of years ago, changes in the earth's climate caused animal and plant life to diversify.\"]\n",
        "\n",
        "# 単語を分割\n",
        "test_texts = [[word for word in document.lower().split()] for document in test_documents]\n",
        "\n",
        "# 既存の辞書を使用して、コーパスを作成\n",
        "test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
        "\n",
        "# トピックを表示\n",
        "for topics_per_document in lda[test_corpus]:\n",
        "    print(\"新しい文書のトピック:\", topics_per_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czuPihqzE7bJ"
      },
      "source": [
        "## 5.Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05-yCfZEZ9Q"
      },
      "source": [
        "### 5-1.Word2Vecのアルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-io1aSTVEgU3"
      },
      "source": [
        "```\n",
        "指定された次元の空間で単語を表現するアルゴリズム\n",
        "```\n",
        "**単語の分散表現(word embedding)**\n",
        "```\n",
        "単語を高次元の実数ベクトルで表現する技術\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5m9gTaeE-_P"
      },
      "source": [
        "### 5-2.Word2Vecの使用方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPaxjfzkFFx4"
      },
      "source": [
        "```\n",
        "from gensim.models import Word2Vec\n",
        "```\n",
        "**解析に使う文書を用意し、前処理を行う**\n",
        "```\n",
        "documents = [\"I quickly put on my red winter jacket, black snow pants, waterproof boots, homemade mittens, and handknit scarf\",\n",
        "              \"The incessant ticking and chiming echoed off the weathered walls of the clock repair shop\",\n",
        "              \"Nervously, I unfolded the wrinkled and stained letter from my long-dead ancestor\",\n",
        "              \"Into the suitcase, I carelessly threw a pair of ripped jeans, my favorite sweater from high school, an old pair of tube socks with stripes, and $20,000 in cash\",\n",
        "              \"The mangy, scrawny stray dog hurriedly gobbled down the grain-free, organic dog food\",\n",
        "              \"The truth is that, apart from breaking news, the modern media will not report anything much after 5 pm\",\n",
        "              \"In terms of the number of people involved, it was a major disaster, and some of us followed the regular daily news bulletins\",\n",
        "              \"The first reports only a few weeks after this devastating earthquake reveal that tourist bookings have dropped drastically\",\n",
        "              \"Low-lying Louisiana stands to suffer some inundation if climate change projections come true\"]\n",
        "\n",
        "# 前処理\n",
        "texts = [[word for word in document.lower().split()] for document in documents]\n",
        "```\n",
        "**Word2Vecモデルの主要なパラメータ**\n",
        "```\n",
        "sg\t1はskip-gram, 0がCBOW\n",
        "size\t隠れ層の次元\n",
        "window\tコンテクストとして認識する前後の単語数を指定\n",
        "min_count\t指定の数以下の出現回数の単語は無視する\n",
        "```\n",
        "**Word2Vec実装例**\n",
        "```\n",
        "model = Word2Vec(texts,  sg=0, size=10, window=5, min_count=1)\n",
        "```\n",
        "**意味が近い単語が表示する**\n",
        "```\n",
        "print(model.most_similar('news'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skm90ln1F9f8"
      },
      "source": [
        "# Word2Vecのインポート\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "documents = [\"I quickly put on my red winter jacket, black snow pants, waterproof boots, homemade mittens, and handknit scarf\",\n",
        "              \"The incessant ticking and chiming echoed off the weathered walls of the clock repair shop\",\n",
        "              \"Nervously, I unfolded the wrinkled and stained letter from my long-dead ancestor\",\n",
        "              \"Into the suitcase, I carelessly threw a pair of ripped jeans, my favorite sweater from high school, an old pair of tube socks with stripes, and $20,000 in cash\",\n",
        "              \"The mangy, scrawny stray dog hurriedly gobbled down the grain-free, organic dog food\",\n",
        "              \"The truth is that, apart from breaking news, the modern media will not report anything much after 5 pm\",\n",
        "              \"In terms of the number of people involved, it was a major disaster, and some of us followed the regular daily news bulletins\",\n",
        "              \"The first reports only a few weeks after this devastating earthquake reveal that tourist bookings have dropped drastically\",\n",
        "              \"Low-lying Louisiana stands to suffer some inundation if climate change projections come true\"]\n",
        "\n",
        "# 大文字を小文字に変換し、単語に分割\n",
        "texts = [[word for word in document.lower().split()] for document in documents]\n",
        "\n",
        "#モデルの作成\n",
        "model = Word2Vec(texts, sg=0, size=10, window=5, min_count=1)\n",
        "\n",
        "# newsに最も近い単語は？\n",
        "print(model.most_similar('news'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usFWODyBFaRY"
      },
      "source": [
        "# [3] 自然言語処理の実践（映画レビュー分類）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOIBS3ZZFaTo"
      },
      "source": [
        "## 1.データの読み込みと目的変数の数値化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4JGWL5HkJo"
      },
      "source": [
        "### 1-1.データの読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjecSaIHpn2"
      },
      "source": [
        "**先頭の5行を表示する**\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "print(df.head(5))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBF35DfCHyl8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "print(df.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nR0aKM7Hzl3"
      },
      "source": [
        "### 1-2.目的変数の分布の確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00juvrhSH7cP"
      },
      "source": [
        "**列に入っているカテゴリごとにデータ数をカウントする**\n",
        "```\n",
        "df['sentiment'].value_counts()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCYP2pt5IFWe"
      },
      "source": [
        "# sentiment列に入っているカテゴリごとにデータ数をカウント\n",
        "print(df['sentiment'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flq9ehxqIRnD"
      },
      "source": [
        "### 1-3.カテゴリ変数の数値化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpPmXRAvIh7g"
      },
      "source": [
        "**カテゴリを数値に変換する**\n",
        "```\n",
        "labels = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvhCtmzCIt0X"
      },
      "source": [
        "labels = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQuUyTjQFaWH"
      },
      "source": [
        "## 2.前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQiYJ7S-I0HJ"
      },
      "source": [
        "### 2-1.前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEvzOa3I5s_"
      },
      "source": [
        "**各文章について以下の処置を施す**\n",
        "```\n",
        "*   URL部分を削除する\n",
        "*   記号や数字を削除する\n",
        "*   正規化（大文字を小文字に変換する）\n",
        "*   文末のピリオドをスペース+ピリオドに変換したのち、文章をスペースごとに単語に分解する\n",
        "*   分解した単語を繋げる\n",
        "```\n",
        "**分割後再びスペースで単語を繋げ、文章に戻す**\n",
        "```\n",
        "import re\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "def cleaning(text):    \n",
        "    #URL部分の削除\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    #記号や数字を削除\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    #正規化（大文字を小文字に変換）\n",
        "    text  = text.lower()\n",
        "    #文末のピリオドをスペース+ピリオドに変換したのち、文章をスペースごとに単語に分解する\n",
        "    text = text.replace('.', ' .').split()\n",
        "    #分解した単語を繋げる\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "clean_text = df['review'].map(cleaning)\n",
        "```\n",
        "**前処理前の文章と前処理後の文章を比較する**\n",
        "```\n",
        "#前処理前\n",
        "df['review'][0]\n",
        "\n",
        "#前処理後\n",
        "clean_text[0]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN8iMmxpJe94"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction import text\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "#クリーニング用関数作成\n",
        "def cleaning(text):\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) #URLの削除\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text) #記号・数字の削除\n",
        "    text  = text.lower() #大文字を小文字に変換\n",
        "    text = text.replace('.', ' .').split() #トークン化\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "#df['review']全体にcleaning関数の処理を行う\n",
        "clean_text = df['review'].map(cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoGmlUR0FaYK"
      },
      "source": [
        "## 3.Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZNCtFg9JpCY"
      },
      "source": [
        "### 3-1.Bag of Wordsの映画レビューデータセットへの適用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIO-FE8AJv67"
      },
      "source": [
        "**ベクトル化する**\n",
        "```\n",
        "# CountVectorizerをインポート\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# max_featuresを指定してインスタンス化\n",
        "bow_vectorizer = CountVectorizer(max_features = 3000) \n",
        "# fit, transformを一括で行う\n",
        "bow = bow_vectorizer.fit_transform(clean_text)\n",
        "# 出力の形状を確認\n",
        "print(bow.shape)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeV3940nJ7MF"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_features = 3000) \n",
        "bow = bow_vectorizer.fit_transform(clean_text)\n",
        "print(\"得られたベクトルの形状:\", bow.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEixARzzFp1I"
      },
      "source": [
        "## 4.モデリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpMUDUFEKFh5"
      },
      "source": [
        "### 4-1.学習用・評価用データの分割"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaq6QVi6KMwE"
      },
      "source": [
        "**学習用データと評価用データの分割する**\n",
        "```\n",
        "train_X, test_X, train_y, test_y = train_test_split(説明変数, 目的変数, test_size = 数値)\n",
        "```\n",
        "**分割比率（評価用データの割合）を指定する**\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_bow_train, x_bow_test, y_bow_train, y_bow_test = train_test_split(bow, labels, test_size=0.5, random_state=42)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsqKYvu9Kb3l"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_bow_train, x_bow_test, y_bow_train, y_bow_test = train_test_split(bow, labels, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD8HYfM5KktU"
      },
      "source": [
        "### 4-2.モデルの学習・予測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLy0MARrKmqE"
      },
      "source": [
        "**XGBoost(eXtreme Gradient Boosting) **\n",
        "```\n",
        "決定木を複数組み合わせて総合的に結果を出す勾配ブースティングアルゴリズムの先進的な実装例で、データサイエンスのコンペでよく使われる手法\n",
        "```\n",
        "**決定木**\n",
        "```\n",
        "*   質問に対する分岐を階層的に作ることで判別を行うモデル\n",
        "*   予測が非常に高速\n",
        "*   基本的に2択の分岐が繰り返される構造のため、可視化が可能で、人間が理解しやすい（ただし木が大きくなると理解が難しくなる）\n",
        "*   数値データとカテゴリデータが混在していても適用可能\n",
        "```\n",
        "**XGBClassifierをインポート**\n",
        "```\n",
        "# max_depthで木の深さの上限、n_estimatorsで学習ラウンド数、n_jobsで計算時の並列スレッド数を指定する\n",
        "# XGBClassifierをインポート\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# モデルの設定\n",
        "mod = XGBClassifier(max_depth=6, n_estimators=5000, n_jobs=-1)\n",
        "#　学習\n",
        "mod.fit(x_bow_train, y_bow_train)\n",
        "```\n",
        "**モデルの推論を行う**\n",
        "```\n",
        "pred = mod.predict(x_bow_test)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeg4QRklLiNz"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "mod = XGBClassifier(max_depth=6, n_estimators=5000, n_jobs=-1)\n",
        "mod.fit(x_bow_train, y_bow_train)\n",
        "pred = mod.predict(x_bow_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsLtUYqXLtyc"
      },
      "source": [
        "### 4-3.モデルの評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmayN9OML1oS"
      },
      "source": [
        "**混同行列を出力する**\n",
        "```\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# 混同行列の出力\n",
        "conf_mx = confusion_matrix(y_bow_test, pred)\n",
        "```\n",
        "**混同行列をpandasのデータフレーム形式に直す**\n",
        "```\n",
        "label_name = ['negative', 'positive']\n",
        "conf_df = pd.DataFrame(data=conf_mx, index=[x + \"(act)\" for x in label_name], columns=[x + \"(pred)\" for x in label_name])\n",
        "conf_df\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWu2fh8MIv3"
      },
      "source": [
        "# confusion_matrixをインポート\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as  pd\n",
        "\n",
        "# 混同行列の作成\n",
        "conf_mx = confusion_matrix(y_bow_test, pred)\n",
        "\n",
        "# データフレームに結果を格納\n",
        "label_name = ['negative', 'positive']\n",
        "conf_df = pd.DataFrame(data=conf_mx, index=[x + \"(act)\" for x in label_name], columns=[x + \"(pred)\" for x in label_name])\n",
        "\n",
        "# データフレームに変換した混同行列を表示\n",
        "print(conf_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}