{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIGNATE_Study.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdzxYbyOhVCmIvvazZjzY/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD36DnCpcc7z"
      },
      "source": [
        "# SIGNATE学習ノート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hBYWWMEcyuX"
      },
      "source": [
        "## **Q1. 自動車環境性能の改善(重回帰、回帰問題)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Me7XGK2YVAA"
      },
      "source": [
        "### **データ分析の流れ・よく使うコマンド**\n",
        "\n",
        "①データの読み込み・確認\n",
        "1. データの読み込み\n",
        "<p>read_csv()\n",
        "\n",
        "2. データ精査\n",
        "<p>df.head()\n",
        "<p>df.shape()\n",
        "\n",
        "3. 欠損値・外れ値の確認\n",
        "<p>df.isnull()\n",
        "<p>df.isnull().sum()\n",
        "<p>ddf.rop(columns='')\n",
        "<p>df.dropna()\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "②データの特徴把握(統計量とデータの可視化)\n",
        "1. 変数の分布・値の確認\n",
        "<p>df.descrive()\n",
        "<p>df.type(data)\n",
        "<p>df.plot.hist(title='')\n",
        "<p>df.value_counts()\n",
        "<p>df.plot.bar(title='')\n",
        "\n",
        "2. 変数間の相関\n",
        "<p>df.corr()\n",
        "<p>sns.heatmap()\n",
        "<p>plt.scatter(,)\n",
        "<p>plt.xlabel('')\n",
        "<p>plt.ylabel('')\n",
        "<p>df.info()\n",
        "<p>sns.boxplot(横軸に割り当てたいカラム, 縦軸に割り当てたいカラム,data=分析対象のデータが代入されているDataFrame)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "③予測モデルの作成\n",
        "1. 学習・評価データの分割\n",
        "<p>df[['カラム1','カラム2','カラム3']]\n",
        "<p>from sklearn.model_selection import train_test_split\n",
        "<p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=, random_state= )\n",
        "\n",
        "2. 評価関数の定義\n",
        "<p>np.array()\n",
        "<p>np.power(a,2) # aを2乗\n",
        "<p>np.sum()\n",
        "<p>len()\n",
        "<p>np.sqrt()\n",
        "<p>from sklearn.metrics import mean_squared_error as MSE\n",
        "MSE(実績値, 予測値)\n",
        "\n",
        "3. モデルの学習\n",
        "<p>from sklearn.linear_model import LinearRegression as LR\n",
        "<p>model = LR()\n",
        "<p>model.fit(X,y)\n",
        "<p>model.predict(X)\n",
        "\n",
        "4. モデルの精度評価\n",
        "<p>RMSE = np.sqrt(MSE(実績値, 予測値))\n",
        "    <p>1. グラフのサイズを指定し、グラフの大きさを正方形になるように設定する\n",
        "    <p>2. 横軸を実測値、縦軸を予測値として、散布図を描く\n",
        "    <p>3. 値域を揃える為に、y_test、y_pred_testの両方を見た上での最小値と最大値を求める\n",
        "    <p>4. 最小値と最大値を使い、x軸およびy軸の値域を指定する\n",
        "    <p>5. 対角線を描く\n",
        "<p>plt.figure(figsize=(横の大きさ, 縦の大きさ))\n",
        "<p>plt.scatter(x,y)\n",
        "<p>np.max(X)\n",
        "<p>np.min(X)\n",
        "<p>np.maximum(A,B)\n",
        "<p>p.minimum(A,B)\n",
        "<p>plt.xlim([xmin, xmax])\n",
        "<p>plt.ylim([ymin, ymax])\n",
        "<p>plt.plot([x1,x2],[y1,y2])\n",
        "<p>plt.xlabel('')\n",
        "<p>plt.ylabel('')\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "④予測モデルの予測精度改善\n",
        "1. ダミー変数化\n",
        "<p>pd.get_dummies(X)\n",
        "    <p>1. 目的変数と説明変数を表す変数を作成し、説明変数については必要があればダミー変数化をする\n",
        "    <p>2. 学習データと評価データに分割する\n",
        "    <p>3. モデルの箱を表す変数を作成し、目的変数・説明変数を用いて、モデルを学習する\n",
        "    <p>4. 学習済みモデルと説明変数を用いて、学習および評価データに対する予測値を算出する\n",
        "    <p>5. それぞれの予測値と評価関数を用いて、モデルの予測精度を評価する\n",
        "\n",
        "2. 対数化\n",
        "<p>np.log(DataFrameを代入した変数['対数化したいカラム名'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqglUbC1dhjl"
      },
      "source": [
        "## **Q2. 健康経営のための疾患リスク予測(ロジスティック回帰、分類)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFvNc3QteKpF"
      },
      "source": [
        "### **データ分析の流れ・よく使うコマンド**\n",
        "①データの読込、検査項目の確認\n",
        "1. データの読み込みと内容確認\n",
        "<p>import ライブラリ名\n",
        "<p>import ライブラリ名 as 省略名\n",
        "<p>import numpy as np import seaborn as sns\n",
        "<p>ライブラリ名.関数()\n",
        "<p>d.read_csv('ファイル名')\n",
        "<p>print(変数）\n",
        "<p>データフレーム名.head(行数)\n",
        "<p>データフレーム.shape\n",
        "<p>データフレーム.info()\n",
        "\n",
        "2. データの整形\n",
        "<p>データフレーム.isnull()\n",
        "<p>データフレーム.isnull().head(行数)\n",
        "<p>データフレーム.isnull().sum()\n",
        "<p>データフレーム.isnull().head()\n",
        "<p>df.isnull().any(axis=1).head()\n",
        "<p>df[df.isnull().any(axis=1)]\n",
        "<p>df.dropna()\n",
        "<p>シリーズ名①.fillna(シリーズ名②, inplace=True)\n",
        "<p>df.loc[行（インデックス）,列（カラム）]\n",
        "<p>df.duplicated()\n",
        "<p>df.duplicated().sum()\n",
        "<p>df[df.duplicated()]\n",
        "<p>df.drop_duplicates(inplace=True)\n",
        "<p>df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "②データの特徴の洗い出し\n",
        "1. 基本統計量、データ数の確認\n",
        "<p>df.describe()\n",
        "<p>df.describe(include='all')\n",
        "<p>df.drop(除外したいカラム名, axis=1)\n",
        "<p>df.value_counts()\n",
        "\n",
        "2. 一変数の可視化\n",
        "<p>import matplotlib.pyplot as plt\n",
        "<p>df[\"\"].value_counts().plot(kind='bar')\n",
        "<p>plt.show()\n",
        "<p>df.hist()\n",
        "<p>plt.tight_layout()\n",
        "<p>df..hist(figsize=(横の大きさ, 縦の大きさ))\n",
        "<p>pd.concat([データフレーム①, データフレーム②], axis=1)\n",
        "<p>df[df[\"カラム名\"] == 条件].head()\n",
        "<p>df.query(\"カラム名 == 条件\").head()\n",
        "<p>df_new = df.query(\"カラム名==条件\")[\"抽出したいカラム名\"]\n",
        "<p>import seaborn as sns\n",
        "<p>sns.distplot(df)\n",
        "<p>plt.legend(labels=[\"凡例\"], loc='upper right')\n",
        "<p>sns.distplot(1つ目のデータフレーム)\n",
        "<p>sns.distplot(2つ目のデータフレーム)\n",
        "<p>plt.legend(labels=[\"1つ目のラベル\", \"2つ目のラベル\"], loc='upper right')\n",
        "<p>plt.xlim(始数, 終数)\n",
        "<p>plt.subplot(縦に並べるプロットの数, 横に並べるプロットの数, プロット番号(何番目のプロットか))\n",
        "<p>plt.figure(figsize=(横の大きさ, 縦の大きさ))\n",
        "\n",
        "3. 相関の可視化\n",
        "<p>df[[\"カラム1\", \"カラム2\"]].corr()\n",
        "<p>df.corr()\n",
        "<p>plt.figure(figsize=(横の大きさ, 縦の大きさ))\n",
        "<p>sns.heatmap(df.corr(), vmin=最小値, vmax=最大値, annot=True, cmap='coolwarm', linewidths=0.1)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "③モデル作成前のデータ前処理\n",
        "1. モデリング用の前処理\n",
        "<p>lambda (任意の変数名): (結果1) if (条件1) else (結果2)\n",
        "<p>df[\"カラム名\"].apply(lambda 変数名: 結果1 if x==条件 else 結果2)\n",
        "\n",
        "2. 学習用・評価用データの分割\n",
        "<p>from sklearn.model_selection import train_test_split\n",
        "<p>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "④モデルの作成\n",
        "1. モデルの学習と予測\n",
        "<p>from sklearn.linear_model import LogisticRegression\n",
        "<p>モデル名.fit(説明変数, 目的変数)\n",
        "<p>変数名 = モデル名.predict(評価用の説明変数データ)\n",
        "<p>変数名 = モデル名.predict_proba(評価用の説明変数データ)\n",
        "<p>変数名 = モデル名.predict_proba(評価用の説明変数データ)[:, 1]\n",
        "<p>sum(変数名 > 条件)\n",
        "\n",
        "2. モデルの評価\n",
        "<p>from sklearn.metrics import confusion_matrix\n",
        "<p>変数名 = confusion_matrix(y_true=実際の正解となるデータ, y_pred=予測結果のデータ)\n",
        "<p>変数名 = pd.DataFrame(np.rot90(混同行列が格納された変数名, 2), index=[\"actual_Positive\", \"actual_Negative\"], columns=[\"predict_Positive\", \"predict_Negative\"])\n",
        "<p>sns.heatmap(変数名, annot=True, fmt=\"2g\", cmap='Blues')\n",
        "<p>from sklearn.metrics import roc_auc_score, roc_curve\n",
        "<p>auc_score = roc_auc_score(y_true=正解データ, y_score=予測確率のデータ)\n",
        "<p>fpr, tpr, thresholds = roc_curve(y_true=正解データ, y_score=予測確率のデータ)\n",
        "<p>plt.plot(fpr, tpr, label='roc curve (area = %0.3f)' % <p>auc_score)\n",
        "<p>plt.plot([0, 1], [0, 1], linestyle=':', label='random')\n",
        "<p>plt.plot([0, 0, 1], [0, 1, 1], linestyle=':', label='ideal')\n",
        "<p>plt.legend()\n",
        "<p>plt.xlabel('false positive rate')\n",
        "<p>plt.ylabel('true positive rate')\n",
        "<p>plt.show()\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "⑤データの特徴を利用したモデル改善\n",
        "<p>1. 特徴量の生成1 （ビニング）\n",
        "<p>変数 = pd.cut(df[\"変数1\"], bins=10)\n",
        "<p>変数1, 変数2(bin_indice) = pd.cut(df[\"変数1\"], bins=10, retbins=True)\n",
        "<p>X_cut = pd.cut(X[\"変数1\"], bins=[境界値1, 境界値2, 境界値3, ...])\n",
        "<p>X_cut = pd.cut(X[\"変数1\"], bins=[境界値1, 境界値2, 境界値3, ...], labels=False)\n",
        "<p>X_dummies = pd.get_dummies(X_cut)\n",
        "<p>X_binned = pd.concat([X, X_dummies], axis=1)\n",
        "<p>X_dummies = pd.get_dummies(X_cut, prefix=X_cut.name)\n",
        "<p>pd.qcut(sr, 3, labels=['ラベル1', 'ラベル2', 'ラベル3'])\n",
        "    <p>1. 学習用・評価用データの分割\n",
        "    <p>2. モデルの学習・予測\n",
        "    <p>3. ROC曲線の描画（偽陽性率、真陽性率、閾値の算出）\n",
        "    <p>4. AUCスコアの算出\n",
        "\n",
        "2. 特徴量の生成2 （多項式特徴量、交互作用特徴量）\n",
        "<p>from sklearn.preprocessing import PolynomialFeatures\n",
        "<p>polynomial = PolynomialFeatures(degree=次数, <p>include_bias=False)\n",
        "<p>polynomial_result = polynomial.fit_transform(データ)\n",
        "\n",
        "3. 特徴量選択\n",
        "<p>from sklearn.feature_selection import SelectFromModel\n",
        "<p>fs_model = LogisticRegression(penalty=\"{正則化オプショ L1やL2}\", random_state=0)\n",
        "<p>fs_threshold = \"{閾値オプション median（中央値）、mean（平均値）、あるいは1.25 × mean（スケーリング係数）}\"\n",
        "<p>selector = SelectFromModel(fs_model, threshold=fs_threshold)\n",
        "<p>モデル.fit(説明変数、目的変数)\n",
        "<p>変数A = selector.get_support()\n",
        "<p>変数B = X_polynomial.loc[:, 変数A]\n",
        "<p>X_train, X_test, y_train, y_test = train_test_split(X_polynomial_masked, y, test_size=0.3, random_state=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBNj6qr0a49B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}