{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Future_Prediction_Of_Cloud_Images.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzeXcGLzC130dyfJIZyqon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya5614/Kaggle/blob/main/Future_Prediction_Of_Cloud_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8dGw-bwUJx"
      },
      "source": [
        "# 雲画像の未来予測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28kwUUS_wUMt"
      },
      "source": [
        "# [1] データの確認・把握"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtKLVgvHwx-u"
      },
      "source": [
        "## 1.衛星画像データの確認・把握"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oivttNpkFNs_"
      },
      "source": [
        "### 1-1.衛星画像データのファイルパス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn1IDhBSFRxC"
      },
      "source": [
        "**ファイルパスとは、ファイルの住所を表す文字列を意味し、以下の2つに大別される**\n",
        "```\n",
        "*   絶対パス: 最上位の階層から見た目的のファイルの場所\n",
        "*   相対パス: プログラムを実行する位置から見た目的のファイルの場所\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rwzt6KUGIuV"
      },
      "source": [
        "### 1-2.衛星画像を読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A44VyxkmGPYT"
      },
      "source": [
        "**画像の読み込むライブラリのインポート**\n",
        "```\n",
        "# OpenCVライブラリのインポート\n",
        "import cv2\n",
        "```\n",
        "**画像を読み込む**\n",
        "```\n",
        "# cv2.imread()関数を使って、白黒画像を読み込む\n",
        "image = cv2.imread(画像のファイルパス, 0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC_MYyYpGqvj"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# 2016年１月１日16時時点の衛星画像を指定するファイルパス\n",
        "file_path = 'train/sat/2016-01-01/2016-01-01-16-00.fv.png'\n",
        "\n",
        "# 衛星画像を読み込む\n",
        "image = cv2.imread(file_path, 0)\n",
        "\n",
        "# 出力\n",
        "print(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vR3AphiG17T"
      },
      "source": [
        "### 1-3.衛星画像を構成する値の統計量を確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhoYKcf7G5Zs"
      },
      "source": [
        "**読み込んだ衛星画像がどのような値で構成されているのかを確認する**\n",
        "```\n",
        "*   最大値　np.max()関数\n",
        "*   最小値　np.min()関数\n",
        "*   平均値　np.mean()関数\n",
        "```\n",
        "```\n",
        "# NumPyライブラリのインポート\n",
        "import numpy as np\n",
        "\n",
        "# 最大値の取得\n",
        "np.max(配列)\n",
        "\n",
        "# 最小値の取得\n",
        "np.min(配列)\n",
        "\n",
        "# 平均値の取得\n",
        "np.mean(配列)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofS-fdfEHQ5-"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# 2016年１月１日16時時点の衛星画像を指定するファイルパス\n",
        "file_path = 'train/sat/2016-01-01/2016-01-01-16-00.fv.png'\n",
        "\n",
        "# 衛星画像を読み込む\n",
        "image = cv2.imread(file_path, 0)\n",
        "\n",
        "# 最大値の出力\n",
        "print(np.max(image))\n",
        "\n",
        "# 最小値の出力\n",
        "print(np.min(image))\n",
        "\n",
        "# 平均値の出力\n",
        "print(np.mean(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3r6OVWqHXL1"
      },
      "source": [
        "### 1-4.衛星画像を可視化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5huK6d1LHfMl"
      },
      "source": [
        "**今回扱う衛星画像**\n",
        "```\n",
        "*   最小値0\n",
        "*   最大値255\n",
        "# 8bit(256階調)\n",
        "```\n",
        "**可視化ライブラリをインポートする**\n",
        "```\n",
        "# matplotlib.pyplotモジュールのインポート\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "**画像を可視化する**\n",
        "```\n",
        "# 白黒画像の描画\n",
        "plt.imshow(画像を表す配列, 'gray')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F00mgKdmP1sm"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2016年１月１日16時時点の衛星画像を指定するファイルパス\n",
        "file_path = 'train/sat/2016-01-01/2016-01-01-16-00.fv.png'\n",
        "\n",
        "# 衛星画像を読み込む\n",
        "image = cv2.imread(file_path, 0)\n",
        "\n",
        "# 衛星画像を描画する\n",
        "plt.imshow(image, 'gray')\n",
        "\n",
        "# グラフの出力\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzV5czR0QCEw"
      },
      "source": [
        "### 1-5.ファイルパスを一般化する① f文字列による文字列の置換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBkXlTo7QKyS"
      },
      "source": [
        "**規則性を捉えるために、文字列の構成成分を理解する**\n",
        "```\n",
        "# 下記のように一般化する\n",
        "{trainもしくはtest}/sat/{年}-{月}-{日}/{年}-{月}-{日}-{時}-00.fv.png\n",
        "\n",
        "衛星画像のファイルパスは\n",
        "*   train もしくは test\n",
        "*   年\n",
        "*   月\n",
        "*   日\n",
        "*   時\n",
        "の5つの成分から構成されています。\n",
        "\n",
        "\n",
        "これらの成分に対応する５つの変数である\n",
        "*   phase:train もしくはtest\n",
        "*   year: 年\n",
        "*   month: 月\n",
        "*   day: 日\n",
        "*   hour: 時間\n",
        "を与える。\n",
        "\n",
        "{phase}/sat/{year}-{month}-{day}/{year}-{month}-{day}-{hour}-00.fv.png\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diRagSnMQ5sM"
      },
      "source": [
        "**f文字列と置換フィールド**\n",
        "```\n",
        "phase = 'train'\n",
        "year = 2016\n",
        "month = 1\n",
        "day = 1\n",
        "hour = 16\n",
        "\n",
        "# f文字列による文字列の置換\n",
        "file_path = f'{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png'\n",
        "\n",
        "## 置換フィールド{}内の文字列が変数によって置換されることで、\n",
        "## file_pathに代入される文字列は、'train/sat/2016-01-01/2016-01-01-16-00.fv.png'となる。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MHto--XRDxa"
      },
      "source": [
        "# 衛星画像のファイルパスを特定するための５つの変数に値を代入する\n",
        "phase = 'train'\n",
        "year = 2016\n",
        "month = 1\n",
        "day = 1\n",
        "hour = 16\n",
        "\n",
        "# 「f文字列」による置換が可能な文字列をfile_pathに代入する\n",
        "file_path = f'{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png'\n",
        "\n",
        "print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EN7Qs5dROzU"
      },
      "source": [
        "### 1-6.ファイルパスを一般化する② 日時の扱い方"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PdZywwFRUkM"
      },
      "source": [
        "**衛星画像の各ファイルパスを分ける差分となる変数について改めて確認する**\n",
        "```\n",
        "*   phase:train もしくはtest\n",
        "*   year: 年\n",
        "*   month: 月\n",
        "*   day: 日\n",
        "*   hour: 時間\n",
        "# year、month、day、hourの４つは全て日時を表すもの\n",
        "# phaseに関しても日時の情報によって文字列を特定できる\n",
        "```\n",
        "**datetimeとtimedelta**\n",
        "```\n",
        "datetime: 日時情報を表す\n",
        "timedelta: 日時どうしの差分(経過時間)を表す\n",
        "```\n",
        "**datetime、timedeltaのインポート**\n",
        "```\n",
        "# datetime.datetimeクラスのインポート\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# datetime.timedeltaクラスのインポート\n",
        "from datetime import timedelta\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1eqNnqRSDx8"
      },
      "source": [
        "**1. datetimeによる日時の操作**\n",
        "\n",
        "```\n",
        "# オブジェクト生成の際の引数に(年, 月, 日, 時, 分, 秒)をそれぞれ指定することで、日時を表すオブジェクトを作成できる\n",
        "\n",
        "# 「2016年1月1日16時0分0秒」\n",
        "date = dt(2016, 1, 1, 16, 0, 0)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yer3h_18SaAj"
      },
      "source": [
        "**年, 月, 日, 時, 分, 秒の情報を、それぞれ単独で数値データとして抽出する**\n",
        "\n",
        "```\n",
        "# 「2016年1月1日16時0分0秒」\n",
        "date = dt(2016, 1, 1, 16, 0, 0)\n",
        "\n",
        "\n",
        "# dateから、「年」のみを数値データとして抽出する\n",
        "## yearに2016が代入される\n",
        "year = date.year\n",
        "\n",
        "# dateから、「月」のみを数値データとして抽出する\n",
        "## monthに1が代入される\n",
        "month = date.month\n",
        "\n",
        "# dateから、「日」のみを数値データとして抽出する\n",
        "## dayに1が代入される\n",
        "day = date.day\n",
        "\n",
        "# dateから、「時」のみを数値データとして抽出する\n",
        "## hourに16が代入される\n",
        "hour = date.hour\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPbjQ8x4SiE4"
      },
      "source": [
        "**2. timedeltaによる日時の加算、減算**\n",
        "\n",
        "```\n",
        "# 「年」「日」「時」「分」「秒」の各単位ごとに、日時の差分を指定できる\n",
        "\n",
        "単位\t引数名\n",
        "年\tyears\n",
        "日\tdays\n",
        "時\thours\n",
        "分\tminutes\n",
        "秒\tseconds\n",
        "\n",
        "# 例) 2時間分の差分を表すtimedeltaオブジェクトを生成する\n",
        "two_hours = timedelta(hours=2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5OKgrr6Su2w"
      },
      "source": [
        "**datetimeオブジェクトの日時情報に対して変更を加える**\n",
        "```\n",
        "# 「2016年1月1日16時0分0秒」のdatetimeオブジェクト\n",
        "date = dt(2016, 1, 1, 16, 0, 0)\n",
        "\n",
        "# 2時間後を表すdatetimeオブジェクトを生成\n",
        "date_after2hours = date + timedelta(hours=2)\n",
        "\n",
        "# 3年前を表すdatetimeオブジェクトを生成\n",
        "date_before3years = date - timedelta(years=3)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLohfCrES5xI"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "\n",
        "# 「2016年1月1日16時0分0秒」の日時オブジェクトを作成する\n",
        "date = dt(2016, 1, 1, 16, 0, 0)\n",
        "\n",
        "# dateの１日後の日時オブジェクトdate_after1daysを作成する\n",
        "date_after1days = date + timedelta(days=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyoJoVXOS-XK"
      },
      "source": [
        "### 1-7.衛星画像を並べて可視化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvg4Fe3pTB_2"
      },
      "source": [
        "**これまでの作業**\n",
        "```\n",
        "1.   f文字列の手法による文字列の置換\n",
        "2.   datetimeモジュールによる日時の操作\n",
        "```\n",
        "**複数の画像を並べて表示する**\n",
        "\n",
        "```\n",
        "# 1枚目の画像を表示する\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(1枚目の画像)\n",
        "\n",
        "# 2枚目の画像を表示する\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(2枚めの画像)\n",
        "\n",
        "# plt.subplot(x, y, z)は、x行y列の図の内、z番目の図に描画することを宣言する\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G6JGHwdTa7y"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,10))\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "# 日時の初期設定として、1つ目の衛星画像に該当する日時を設定する\n",
        "start_date = dt(2016, 1, 1, 16)\n",
        "\n",
        "# 5時間分の衛星画像を順番に取得、描画する\n",
        "for i in range(5):\n",
        "    \n",
        "    \"\"\"\n",
        "    2. 日時の操作\n",
        "    \"\"\"\n",
        "    # start_dateから、i時間先の日時をdateに代入する\n",
        "    date = start_date + timedelta(hours=i)\n",
        "    \n",
        "    # 年:year、月:month、日:day、時:hourの情報をdateから取得する\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    hour = date.hour\n",
        "    \n",
        "    # 2018年ならばテストデータ(test)\n",
        "    # 2016年、2017年ならば学習データ(train)をphaseに代入する\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    else:\n",
        "        phase = 'train'\n",
        "        \n",
        "    \"\"\"\n",
        "    1. 文字列の置換\n",
        "    \"\"\"\n",
        "    # ファイル名を指定する\n",
        "    file_path = f\"{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png\"\n",
        "    \n",
        "    # 衛星画像を読み込む\n",
        "    image = cv2.imread(file_path)\n",
        "        \n",
        "    # 1行5列並んだ図の内、i+1番目の図に描画する設定を行う \n",
        "    plt.subplot(1, 5, i+1)\n",
        "    \n",
        "    # 画像を描画する\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slD6BhXdwyCN"
      },
      "source": [
        "## 2.気象データの確認・把握"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cA0GTHVKLA"
      },
      "source": [
        "### 2-1.気象データのファイルパス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_vEND3yWqXD"
      },
      "source": [
        "**各ディレクトリ名についての説明**\n",
        "```\n",
        "1.   train と test\n",
        "学習用(train)として2016年&2017年、評価用(test)として2018年のデータを使用するため、2016年、2017年のデータはtrainディレクトリ内に、2018年のデータはtestディレクトリ内に含まれている。\n",
        "\n",
        "2.   sat と met\n",
        "衛星(Satellite)画像データを「sat」気象(Meteorological)データを「met」と、それぞれ表現している。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrlmVsNKXIWs"
      },
      "source": [
        "### 2-2.気象データを読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k46WEZjXNf8"
      },
      "source": [
        "**gzipファイルを扱う方針**\n",
        "```\n",
        "1.   ストレージ内に全てのgzipファイルを展開してしまう\n",
        "# 読み込み処理にかかる時間が削減される一方、ストレージの容量が圧迫される\n",
        "\n",
        "2.   気象データを読み込むタイミングで毎回gzipファイルを開く\n",
        "# 読み込み処理にかかる時間がかかる一方、ストレージの容量が圧迫されない\n",
        "```\n",
        "**gzipファイルを引数に受け取り、NumPyファイルを返り値とする関数**\n",
        "```\n",
        "def Read_gz_Binary(file_path):\n",
        "    file_tmp = file + \"_tmp\"\n",
        "    with gzip.open(file, 'rb') as f_in:\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    bin_data = np.fromfile(file_tmp, np.float32)\n",
        "    os.remove(file_tmp)\n",
        "    return bin_data.reshape( [168,128] )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoNZ6TdjZkIU"
      },
      "source": [
        "**Read_gz_Binary関数**\n",
        "\n",
        "```\n",
        "*   ファイルパスを引数に受け取り\n",
        "*   形状が(168, 128)のNumPy配列を返り値とする\n",
        "```\n",
        "**Read_gz_Binary関数内部の処理**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-aglZIKZv_u"
      },
      "source": [
        "**1.「gzip.open()」関数によるgzファイルの読み込み**\n",
        "```\n",
        "# gzipモジュールのインポート\n",
        "import gzip\n",
        "\n",
        "# gzipファイルをバイナリデータで開いたファイルオブジェクトとして返す\n",
        "with gzip.open(ファイルパス, 'rb') as opened_file:\n",
        "\n",
        "# 第1引数に、展開したいファイルのファイルパスを指定\n",
        "# 第2引数は、'r'が読み込み、'b'がバイナリモードを意味する\n",
        "```\n",
        "**2.「shutil.copyfileobj()」関数によるファイルオブジェクトのコピー**\n",
        "\n",
        "```\n",
        "# shutilのインポート\n",
        "import shutil\n",
        "\n",
        "# ファイルオブジェクト1の中身を、ファイルオブジェクト2にコピーする\n",
        "shutil.copyfileobj(ファイルオブジェクト1, ファイルオブジェクト2)\n",
        "```\n",
        "**「np.fromfile()」関数によるバイナリデータの読み込み**\n",
        "```\n",
        "# NumPyライブラリのインポート\n",
        "import numpy as np\n",
        "\n",
        "# バイナリファイルをNumPy配列として読み込む\n",
        "bin_data = np.fromfile(ファイルパス, dtypeの指定)\n",
        "\n",
        "# 第1引数にバイナリファイルのパス\n",
        "# 第2引数に返り値となるNumPy配列のdtypeを指定する\n",
        "```\n",
        "**4. 「np.reshape()」関数による配列の2次元化**\n",
        "```\n",
        "# バイナリデータ(1次元の配列)を\n",
        "# 2次元の配列に変換する\n",
        "array2d = bin_data.reshape( [168,128] )\n",
        "```\n",
        "**Read_gz_Binary()関数の処理の流れ**\n",
        "```\n",
        "1.   「gzip.open()」関数によるgzファイルの読み込み\n",
        "1.   「shutil.copyfileobj()」関数によるファイルオブジェクトのコピー\n",
        "3.   「np.fromfile()」関数によるバイナリデータの読み込み\n",
        "```\n",
        "```\n",
        "def Read_gz_Binary(file_path):\n",
        "\n",
        "    # 1. 変数file_tmpに「元のファイル名_tmp」となる文字列を代入\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "\n",
        "    # 2. gzipファイルを、「読み込み(r)モード & バイナリ(b)モード」 で開き、\n",
        "    #     開いたファイルオブジェクトをf_inに代入する\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "\n",
        "        # 3. ファイル名を「元のファイル名_tmp」としたファイルを\n",
        "        #     open()関数の「書き込み(w)モード & バイナリ(b)モード」で新規作成し、\n",
        "        #     ファイルオブジェクトをf_outとする\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "\n",
        "            # 4. f_inのファイルオブジェクトをf_outにコピーする\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # 5. バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "\n",
        "    # 6. 一時的に作成した展開済みのバイナリファイル「元のファイル名_tmp」を削除する\n",
        "    os.remove(file_tmp)\n",
        "\n",
        "    # 7. 読み込んだ配列の形状を(168, 128)に整形する\n",
        "    met_data = met_data.reshape( [168,128] )\n",
        "\n",
        "    return met_data\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ym7Xoh_bRa1"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# 気象データを読み込む関数; Read_gz_Binary を実装する\n",
        "def Read_gz_Binary(file_path):\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    \n",
        "    # バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "    os.remove(file_tmp)\n",
        "    \n",
        "    # 配列の次元に変更を加える\n",
        "    met_data = met_data.reshape( [168,128] )\n",
        "    \n",
        "    return met_data\n",
        "\n",
        "# 気象データのファイルパスを指定する\n",
        "file_path = 'train/met/2016/01/01/HGT.200.3.2016010118.gz'\n",
        "\n",
        "# 気象データを読み込む\n",
        "met_data = Read_gz_Binary(file_path)\n",
        "\n",
        "# 読み込んだ気象データに関する基本情報を出力する\n",
        "print(type(met_data))\n",
        "print(met_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8HygcOObXoF"
      },
      "source": [
        "### 2-3.気象データを構成する値の統計量を確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEzdIiYBbdGf"
      },
      "source": [
        "**算出する統計量**\n",
        "\n",
        "```\n",
        "*   最大値\n",
        "*   最小値\n",
        "*   平均値\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCCDwym6bnx-"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "\n",
        "# 気象データのファイルパスを指定する\n",
        "file_path = 'train/met/2016/01/01/HGT.200.3.2016010118.gz'\n",
        "\n",
        "# 気象データを読み込む\n",
        "met_data = Read_gz_Binary(file_path)\n",
        "\n",
        "# 最大値の出力\n",
        "print('最大値: ', np.max(met_data))\n",
        "\n",
        "# 最小値の出力\n",
        "print('最小値: ', np.min(met_data))\n",
        "\n",
        "# 平均値の出力\n",
        "print('平均値: ', np.mean(met_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G873tdaejoh"
      },
      "source": [
        "### 2-4.データの未計測部分を補間する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGdBQihnerD9"
      },
      "source": [
        "**1. 北側、南側の未計測部分を補間する(上下方向の補間)**\n",
        "```\n",
        "未計測部分\n",
        "*   北側: 1行目と2行目\n",
        "*   南側: 155行目〜168行目\n",
        "\n",
        "有効な計測値\n",
        "*   北側: 3行目\n",
        "*   南側: 154行目\n",
        "\n",
        "\n",
        "# 北側の未計測部分を補間する\n",
        "data[0:2] = data[2]\n",
        "\n",
        "# 南側の未計測部分を補間する\n",
        "data[154:] = data[153]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOi8WvTrlV6c"
      },
      "source": [
        "**2. 西側の未計測部分を補間する(左右方向の補間)**\n",
        "```\n",
        "未計測部分\n",
        "*   西側: 1列目〜8列目\n",
        "\n",
        "有効な計測値\n",
        "*   西側: 9列目\n",
        "\n",
        "\n",
        "# 西側の未計測部分を補間する\n",
        "data[:, :8] = data[:, 8].reshape(-1, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md6GbFVwl8mz"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "\n",
        "# 気象データのファイルパスを指定する\n",
        "file_path = 'train/met/2016/01/01/HGT.200.3.2016010118.gz'\n",
        "\n",
        "# 気象データを読み込む\n",
        "met_data = Read_gz_Binary(file_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(met_data, 'gray')\n",
        "\n",
        "# fill_lack_data()関数を実装する\n",
        "def fill_lack_data(data):\n",
        "    \n",
        "    ## 1. 北側、南側の未計測部分を補間する(上下)\n",
        "    \n",
        "    # 北側の未計測部分を補間する\n",
        "    data[0:2] = data[2]\n",
        "    # 南側の未計測部分を補間する\n",
        "    data[154:] = data[153]\n",
        "    \n",
        "    \n",
        "    ## 2. 西側の未計測部分を補間する(左右)\n",
        "    \n",
        "    # 西側の未計測部分を補間する\n",
        "    data[:, :8] = data[:, 8].reshape(-1, 1)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# fill_lack_data()関数をmet_dataに対して実行する\n",
        "filled_data = fill_lack_data(met_data)\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(filled_data, 'gray')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIm9QvK5mTXP"
      },
      "source": [
        "### 2-5.ファイルパスを一般化する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR84QP6_mb2F"
      },
      "source": [
        "**構成成分の確認**\n",
        "```\n",
        "{train もしくは test}/met/{年}/{月}/{日}/{気象データの名称}.3.{年}{月}{日}{時}.gz\n",
        "\n",
        "衛星画像データと同じく\n",
        "*   train もしくは test\n",
        "*   年\n",
        "*   月\n",
        "*   日\n",
        "*   時\n",
        "の5つに加えて、\n",
        "*   気象データの名称\n",
        "```\n",
        "**気象データの種類名**\n",
        "```\n",
        "*   アルファベット名: 気象データの種類\n",
        "*   数字: 等圧面における気圧\n",
        "\n",
        "\n",
        "*   各アルファベット名と各種気象データの対応\n",
        "アルファベット名\t気象データの種類\n",
        "HGT\t高度\n",
        "PRMSL\t海面気圧\n",
        "RH\t湿度\n",
        "TMP\t気温\n",
        "UGRD\t東西風\n",
        "VGRD\t南北風\n",
        "VVEL\t鉛直流\n",
        "```\n",
        "**それぞれの変数の対応**\n",
        "```\n",
        "*   phase: train もしくは test\n",
        "*   year: 年\n",
        "*   month: 月\n",
        "*   day: 日\n",
        "*   hour: 時\n",
        "*   data_type: 気象データの名称\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsMtPecQnfQZ"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# 日時オブジェクトの作成\n",
        "date = dt(2016, 1, 1, 18)\n",
        "\n",
        "\"\"\"\n",
        "file_path = 'train/met/2016/01/01/HGT.200.3.2016010118.gz'\n",
        "\n",
        "となるように、以下の変数phase, year, month, day, hour, data_typeに値を代入しましょう。\n",
        "\"\"\"\n",
        "\n",
        "phase = 'train'\n",
        "year = date.year\n",
        "month = date.month\n",
        "day = date.day\n",
        "hour = date.hour\n",
        "data_type = 'HGT.200'\n",
        "\n",
        "\n",
        "\n",
        "# f文字列を利用して、ファイルパスを表す文字列を作成する\n",
        "file_path = f'{phase}/met/{year}/{month:02}/{day:02}/{data_type}.3.{year}{month:02}{day:02}{hour:02}.gz'\n",
        "\n",
        "\n",
        "print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_99JFzHwUQK"
      },
      "source": [
        "# [2] データセットの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO4uKBVbw_HN"
      },
      "source": [
        "## 1.データが与えられている日付の取得"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF5xwpPiBJNr"
      },
      "source": [
        "### 1-1.学習用データが与えられている日付を取得する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSyySrP0BNG3"
      },
      "source": [
        "**日時の操作**\n",
        "```\n",
        "# datetime timedeltaのインポート\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "\n",
        "# 日時データの作成 「2016年1月1日16時」\n",
        "date = dt(2016, 1, 1, 16)\n",
        "\n",
        "# 日時データの演算 (1日後)\n",
        "date_after1day = date + timedelta(days=1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLHDC0RABVh-"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "\n",
        "# 学習データが提供されている日の日付を取得する\n",
        "def get_train_days_list():\n",
        "    \n",
        "    # 2016年１月１日１６時からスタート\n",
        "    start_date = dt(2016, 1, 1, 16)\n",
        "    \n",
        "    # 日付を格納するリストを初期化する\n",
        "    days_list = []\n",
        "    \n",
        "    # 学習データは365日×２年分与えられるため、合計730日分の日時を取得する\n",
        "    for i in range(2 * 365):\n",
        "        \n",
        "        # start_dateから、i日後の日時を取得し、dateに代入する\n",
        "        date = start_date + timedelta(days=i)\n",
        "        \n",
        "        # days_listにdateを追加する\n",
        "        days_list.append(date)\n",
        "        \n",
        "    return days_list\n",
        "\n",
        "# 関数を実行して、学習データが提供されている日の日付を取得する\n",
        "train_days_list = get_train_days_list()\n",
        "\n",
        "\n",
        "\n",
        "print(train_days_list[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZskiE59rBaoA"
      },
      "source": [
        "### 1-2.テスト用データが与えられている日付を取得する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QsWVO06Bn8m"
      },
      "source": [
        "**文字列を日付データに変換する**\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# OpenData_startカラム内の全レコードを日付型のデータに変換\n",
        "test_terms['OpenData_start'] = pd.to_datetime(test_terms['OpenData_start'])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL69xytxBv6L"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import pandas as pd\n",
        "\n",
        "# テスト対象期間の日付を取得する関数\n",
        "def get_test_days_list():\n",
        "    # test_terms.csvからtest対象期間の日付を取得\n",
        "    test_terms = pd.read_csv('test_terms.csv')\n",
        "    \n",
        "    # OpenData_startカラムのデータを日付型のデータとして取得する\n",
        "    days_list = pd.to_datetime(test_terms['OpenData_start'])\n",
        "            \n",
        "    return days_list\n",
        "\n",
        "test_days_list = get_test_days_list()\n",
        "print(test_days_list[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzO26BF7w_KW"
      },
      "source": [
        "## 2.衛星画像データをまとめたnpyファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaWLlamTB3SS"
      },
      "source": [
        "### 2-1.衛星画像データの読み込みを行う関数を実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE9iwskxB_L7"
      },
      "source": [
        "**衛星画像データを読み込む関数get_sat_data()**\n",
        "```\n",
        "引数: 日時\n",
        "返り値: 衛星画像データを表すNumPy配列\n",
        "```\n",
        "**ファイルパスの特定**\n",
        "```\n",
        "# 衛星画像データのファイルパス\n",
        "file_name = f\"{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png\"\n",
        "```\n",
        "**一般化されたファイルパスに含まれる変数**\n",
        "```\n",
        "*   phase\n",
        "*   year\n",
        "*   month\n",
        "*   day\n",
        "*   hour\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vHsmRi4Ci15"
      },
      "source": [
        "# 衛星画像データを読み込む関数get_sat_dataの作成\n",
        "def get_sat_data(date):\n",
        "    \n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    hour = date.hour\n",
        "    \n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "        \n",
        "    # ファイル名を指定する\n",
        "    file_name = f\"{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png\"\n",
        "    \n",
        "    # 画像の読み込み\n",
        "    sat_data = cv2.imread(file_name, 0)\n",
        "    \n",
        "    return sat_data\n",
        "\n",
        "# 「2016年1月1日16時」時点の衛星画像を読み込む\n",
        "sat_data = get_sat_data(date=dt(2016, 1, 1 ,16))\n",
        "\n",
        "\n",
        "print(sat_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR6INWhyC8oo"
      },
      "source": [
        "### 2-2.npyファイル作成の方針(衛星画像データ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDcPanl8DL5J"
      },
      "source": [
        "**衛星画像データを構成する5次元配列**\n",
        "```\n",
        "(サンプル数(対象日の日数), 時系列の長さ(24時間), 高さ, 幅, チャンネル数)\n",
        "```\n",
        "**1. 対象日の日数の違いについて**\n",
        "```\n",
        "学習用データ: 2016年、2017年の合計730日分\n",
        "テストデータ: 2018年内から選ばれた50日分\n",
        "```\n",
        "**2. 画像の高さ、幅について**\n",
        "\n",
        "```\n",
        "画像分類タスクで使用されることの多いデータセットの画像サイズの例\n",
        "\n",
        "*   ImageNet: (224, 224)\n",
        "*   CIFAR-10: (32, 32)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzcLA_-qDwp3"
      },
      "source": [
        "### 2-3.衛星画像データをまとめたnpyファイルを作成する関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcJTIMMD5O0"
      },
      "source": [
        "**make_sat_dataset()**\n",
        "\n",
        "```\n",
        "# 引数resizeとphaseを設定\n",
        "*   オリジナルの画像を何分の1サイズに縮小するか(resize)\n",
        "*   学習用データセットを作成するかorテスト用データセットを作成するか(phase)\n",
        "```\n",
        "**データセット作成時の詳しい処理**\n",
        "```\n",
        "def make_sat_dataset(resize, phase):\n",
        "    # 引数phaseの値に従って、\n",
        "    # 学習用、もしくはテスト用のデータが与えられている日付を取得する\n",
        "    ## [get_train_days_list()、get_test_days_list()はともにタスク1で作成した関数]\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # データセットを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    # start_date_listから、順番にデータが与えられている日付を取得し、\n",
        "    # 変数start_dateに代入する\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        # 一日分の衛星画像データを格納するリストの初期化\n",
        "        data_in_1day = []\n",
        "\n",
        "        # 24時間分の衛星画像を順番に取得する\n",
        "        for i in range(24):\n",
        "            # start_dateから、時間を「i時間」分進める\n",
        "            date = start_date + timedelta(hours=i)\n",
        "\n",
        "            # 与えられた日時(date)における衛星画像データを読み込む\n",
        "            ## [get_sat_data()関数は当タスク内で作成した関数]\n",
        "            sat_data = get_sat_data(date)\n",
        "\n",
        "            # 引数resizeの値に従って、データを縮小する\n",
        "            resized_data = cv2.resize(sat_data, ( int(512 / resize), int(672 / resize) ), \n",
        "                                                          interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # data_in_1dayリストへの値の追加\n",
        "            data_in_1day.append(resized_data)\n",
        "\n",
        "        # 1日分(24時間分)のデータを格納したリストをNumPy配列に変換する\n",
        "        # 形状は(24, 高さ, 幅, 1)\n",
        "        data_in_1day = np.array(data_in_1day, dtype='uint8').reshape(24, int(672 / resize), int(512 / resize), 1)\n",
        "\n",
        "        # 取得した1日分のNumPy配列をdatasetリストに追加する\n",
        "        dataset.append(data_in_1day)\n",
        "\n",
        "    # 対象期間に含まれる全データを格納したリストをNumPy配列に変換する\n",
        "    # 形状は(対象期間の日数, 24, 高さ, 幅, 1)\n",
        "    dataset = np.array(dataset, dtype='uint8').reshape(len(start_date_list), 24, int(672 / resize), int(512 / resize), 1)\n",
        "\n",
        "    # 保存するnpyファイル名を指定する\n",
        "    save_name = f'{phase}_sat.npy'\n",
        "\n",
        "    # NumPy配列をnpyファイルとして保存する\n",
        "    np.save(save_name, dataset)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZYjQ3K3Ebtb"
      },
      "source": [
        "**cv2.resize()関数: 画像のリサイズ**\n",
        "```\n",
        "# 画像のリサイズ\n",
        "resized_image = cv2.resize(画像を表す配列, (リサイズ後の画像の幅, リサイズ後の画像の高さ), interpolation=cv2.INTER_AREA)\n",
        "```\n",
        "**np.save(): NumPy配列をファイルとして保存する**\n",
        "```\n",
        "# NumPy配列をファイルとして保存する\n",
        "np.save(ファイル名, 保存の対象となるNumPy配列)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdyaLX0xEq4H"
      },
      "source": [
        "def make_sat_dataset(resize, phase):\n",
        "    \n",
        "    if phase == 'train':\n",
        "        # 学習用データが与えられている日付のリストを取得する\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        # テスト用データが与えられている日付のリストを取得する\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    dataset = []\n",
        "        \n",
        "    for start_date in start_date_list:\n",
        "        \n",
        "        data_in_1day = []\n",
        "        \n",
        "        for i in range(24):\n",
        "            date = start_date + timedelta(hours=i)\n",
        "            sat_data = get_sat_data(date)\n",
        "            \n",
        "            # データの縮小を実行する\n",
        "            resized_data = cv2.resize(sat_data, ( int(512 / resize), int(672 / resize) ), \n",
        "                                                          interpolation=cv2.INTER_AREA)\n",
        "            data_in_1day.append(resized_data)\n",
        "            \n",
        "        data_in_1day = np.array(data_in_1day, dtype='uint8').reshape(24, int(672 / resize), int(512 / resize), 1)\n",
        "        dataset.append(data_in_1day)\n",
        "        \n",
        "    dataset = np.array(dataset, dtype='uint8').reshape(len(start_date_list), 24, int(672 / resize), int(512 / resize), 1)\n",
        "    save_name = f'{phase}_sat.npy'\n",
        "    \n",
        "    # npyファイルの保存を実行する\n",
        "    np.save(save_name, dataset)\n",
        "    \n",
        "    \n",
        "# 「縦横20分の1サイズに縮小した」 「学習用衛星画像データ」を\n",
        "# npyファイルとしてまとめて保存する \n",
        "make_sat_dataset(resize=20, phase='train')\n",
        "\n",
        "\n",
        "\n",
        "sat_dataset = np.load('train_sat.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVtIcrI9w_NH"
      },
      "source": [
        "## 3.気象データをまとめたnpyファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmCbZVVE2aY"
      },
      "source": [
        "### 3-1.気象データを読み込む関数を実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Fshpg7FGeS"
      },
      "source": [
        "**与えられた日時における気象データを読み込む関数**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0IfoljyFHb9"
      },
      "source": [
        "# 「日付(date)」と「気象データの種類(data_type)」を指定して、気象データを読み込む関数\n",
        "def get_met_data(date, data_type):\n",
        "    \n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    hour = date.hour\n",
        "    \n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "        \n",
        "    # ファイル名を指定する\n",
        "    file_name = f'{phase}/met/{year}/{month:02}/{day:02}/{data_type}.3.{year}{month:02}{day:02}{hour:02}.gz'\n",
        "    \n",
        "    # データの読み込み(Read_gz_Binary) -> 空白箇所の穴埋め(fill_lack_data)の順番で処理を行う\n",
        "    met_data = fill_lack_data(Read_gz_Binary(file_name))\n",
        "    \n",
        "    return met_data\n",
        "\n",
        "# get_met_data()関数を実行し、\n",
        "# 2016年1月1日18時の「'HGT.200'」に関する気象データを読み込む\n",
        "met_data = get_met_data(date=dt(2016, 1, 1, 18), data_type='HGT.200')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwq-jLxMFdN2"
      },
      "source": [
        "### 3-2.npyファイル作成の方針(気象データ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amj6vlmQFjSO"
      },
      "source": [
        "**大まかな処理の流れ**\n",
        "```\n",
        "1.   データが3時間ごとに提供されている\n",
        "2.   データが34種類に分けて提供されている\n",
        "```\n",
        "**1. データが3時間ごとに提供されていることへの対応**\n",
        "```\n",
        "*   衛星画像データ: 1時間ごと\n",
        "*   気象データ: 3時間ごと\n",
        "\n",
        "# 3時間間隔で提供されている気象データを、間に生じた隙間を補間することで1時間間隔のデータに変える\n",
        "```\n",
        "**2. データが34種類に分けて提供されていることへの対応**\n",
        "```\n",
        "# 気象データの種類を表す文字列を格納したリスト\n",
        "met_data_type_list = ['HGT.200', 'HGT.300', 'HGT.500', 'HGT.700', 'HGT.850', \n",
        "                         'PRMSL.msl', \n",
        "                         'RH.1p5m', 'RH.300', 'RH.500', 'RH.700', 'RH.850', \n",
        "                         'TMP.1p5m', 'TMP.200', 'TMP.300', 'TMP.500', 'TMP.700', 'TMP.850',\n",
        "                         'UGRD.10m','UGRD.200', 'UGRD.300', 'UGRD.500', 'UGRD.700', 'UGRD.850', \n",
        "                         'VGRD.10m', 'VGRD.200', 'VGRD.300', 'VGRD.500', 'VGRD.700', 'VGRD.850', \n",
        "                         'VVEL.200', 'VVEL.300', 'VVEL.500', 'VVEL.700', 'VVEL.850']\n",
        "```\n",
        "```\n",
        "# forループでリストに含まれる文字列を順に読み込む\n",
        "for data_type in met_data_type_list:\n",
        "\n",
        "    # 読み込んだ引数をget_met_data関数の引数として受け渡し、\n",
        "    # 該当する気象データを読み込む\n",
        "    met_data = get_met_data(date=date, data_type=data_type)\n",
        "\n",
        "    ### 以下にひとつひとつの気象データに対して実行する処理を記述する\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j8akiRgGYI0"
      },
      "source": [
        "### 3-3.気象データをまとめたnpyファイルを作成する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhnxRc8xGbmH"
      },
      "source": [
        "**関数内の大まかな処理の流れ**\n",
        "\n",
        "```\n",
        "resize: オリジナルの画像を何分の1サイズに縮小するか\n",
        "phase: 学習用データを作成するかorテスト用データを作成するか\n",
        "```\n",
        "```\n",
        "# 気象データの種類を表す文字列を格納したリスト\n",
        "met_data_type_list = ['HGT.200', 'HGT.300', 'HGT.500', 'HGT.700', 'HGT.850', \n",
        "                     'PRMSL.msl', \n",
        "                     'RH.1p5m', 'RH.300', 'RH.500', 'RH.700', 'RH.850', \n",
        "                     'TMP.1p5m', 'TMP.200', 'TMP.300', 'TMP.500', 'TMP.700', 'TMP.850',\n",
        "                     'UGRD.10m','UGRD.200', 'UGRD.300', 'UGRD.500', 'UGRD.700', 'UGRD.850', \n",
        "                     'VGRD.10m', 'VGRD.200', 'VGRD.300', 'VGRD.500', 'VGRD.700', 'VGRD.850', \n",
        "                     'VVEL.200', 'VVEL.300', 'VVEL.500', 'VVEL.700', 'VVEL.850']\n",
        "```\n",
        "**HGT(高度)に関する特徴量のみを採用**\n",
        "```\n",
        "# 高度に関する気象データのみを抽出したリスト\n",
        "HGT_list = ['HGT.200', 'HGT.300', 'HGT.500', 'HGT.700', 'HGT.850']\n",
        "```\n",
        "**関数make_met_dataset()**\n",
        "```\n",
        "def make_met_dataset(resize, phase, data_type_list):\n",
        "\n",
        "    if phase == 'train':\n",
        "        start_date_list = train_days_list\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # 全種類、全日時分のデータを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        # 引数data_type_listから、\n",
        "        # 気象データの種類を示す文字列を1つずつ取り出す。\n",
        "        for count, data_type in enumerate(data_type_list):\n",
        "\n",
        "            # 1種類、1日分のデータを格納するリストの初期化\n",
        "            one_type_in_1day = []\n",
        "\n",
        "            for i in range(24):\n",
        "\n",
        "                date = start_date + timedelta(hours=i)\n",
        "                hour = date.hour\n",
        "\n",
        "                \"\"\"データ補間処理の方針\n",
        "                A: \n",
        "                3で割り切ることのできる時刻T(0時, 3時, 6時, 9時, 12時, 15時, 18時, 21時)は、\n",
        "                気象データが提供されているため、単純にデータを読み込む。\n",
        "\n",
        "                B: \n",
        "                それ以外の時刻T+1とT+2については、\n",
        "                時刻Tと時刻T+3時点の気象データを利用して、値を補間する。\n",
        "\n",
        "                \"\"\"\n",
        "\n",
        "                # 時刻がT[0, 3, 6, 9, 12, 15, 18, 21]時の場合\n",
        "                if hour % 3 == 0:\n",
        "\n",
        "                    met_data = get_met_data(date, data_type)\n",
        "\n",
        "                # 時刻がT+1[1, 4, 7, 10, 13, 16, 19, 22]時の場合\n",
        "                elif hour % 3 == 1:\n",
        "\n",
        "                    # １時間前(時刻T)の気象データを読み込む\n",
        "                    before_1h = date - timedelta(hours=1)\n",
        "                    data_before_1h = get_met_data(before_1h, data_type)\n",
        "\n",
        "                    # ２時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_2h = date + timedelta(hours=2)\n",
        "                    data_after_2h = get_met_data(after_2h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (2/3) * data_before_1h + (1/3) * data_after_2h\n",
        "\n",
        "                # 時刻がT+2[2, 5, 8, 11, 14, 17, 20, 23]時の場合\n",
        "                else:\n",
        "\n",
        "                    # ２時間前(時刻T)の気象データを読み込む\n",
        "                    before_2h = date - timedelta(hours=2)\n",
        "                    data_before_2h = get_met_data(before_2h, data_type)\n",
        "\n",
        "                    # １時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_1h = date + timedelta(hours=1)\n",
        "                    data_after_1h = get_met_data(after_1h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (1/3) * data_before_2h + (2/3) * data_after_1h\n",
        "\n",
        "                # 引数resizeの値に従って、データを縮小する\n",
        "                resized_data = cv2.resize(met_data, (int(512 / resize), int(672 / resize)),\n",
        "                                                              interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                # 1種類、1時刻分のデータをone_type_in_1dayリストへ追加する\n",
        "                one_type_in_1day.append(resized_data)\n",
        "\n",
        "            one_type_in_1day = np.array(one_type_in_1day).reshape(24, int(672 / resize), int(512 / resize), 1)\n",
        "\n",
        "            # 1種類目はall_type_in_1dayに代入する\n",
        "            if count == 0:\n",
        "                all_type_in_1day = one_type_in_1day\n",
        "            # 2種類目以降はall_type_in_1dayにチャンネルの次元で結合する\n",
        "            else:\n",
        "                all_type_in_1day = np.concatenate([all_type_in_1day, one_type_in_1day], axis=3)\n",
        "\n",
        "        # 全種類、1日分のデータをdatasetリストへ追加する\n",
        "        dataset.append(all_type_in_1day)\n",
        "\n",
        "    dataset = np.array(dataset, dtype='float32').reshape(len(start_date_list), 24, int(672 / resize), int(512 / resize), len(data_type_list))\n",
        "    save_name = f'{phase}_met.npy'\n",
        "    np.save(save_name, dataset)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pj2iD1qG6Iy"
      },
      "source": [
        "def make_met_dataset(resize, phase, data_type_list):\n",
        "\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()        \n",
        "        \n",
        "    dataset = []    \n",
        "    \n",
        "    for start_date in start_date_list:\n",
        "        \n",
        "        for count, data_type in enumerate(data_type_list):\n",
        "        \n",
        "            one_type_in_1day = []\n",
        "            \n",
        "            for i in range(24):\n",
        "                \n",
        "                date = start_date + timedelta(hours=i)\n",
        "                hour = date.hour\n",
        "                \n",
        "                # セクションA #\n",
        "                if hour % 3 == 0:\n",
        "                    met_data = get_met_data(date, data_type)\n",
        "                elif hour % 3 == 1:\n",
        "                    before_1h = date - timedelta(hours=1)\n",
        "                    data_before_1h = get_met_data(before_1h, data_type)\n",
        "                    after_2h = date + timedelta(hours=2)\n",
        "                    data_after_2h = get_met_data(after_2h, data_type)\n",
        "                    met_data = (2/3) * data_before_1h + (1/3) * data_after_2h\n",
        "                else:\n",
        "                    before_2h = date - timedelta(hours=2)\n",
        "                    data_before_2h = get_met_data(before_2h, data_type)\n",
        "                    after_1h = date + timedelta(hours=1)\n",
        "                    data_after_1h = get_met_data(after_1h, data_type)\n",
        "                    met_data = (1/3) * data_before_2h + (2/3) * data_after_1h\n",
        "                # セクションA #\n",
        "               \n",
        "                resized_data = cv2.resize(met_data, (int(512 / resize), int(672 / resize)),\n",
        "                                                              interpolation=cv2.INTER_AREA)\n",
        "                one_type_in_1day.append(resized_data)\n",
        "                     \n",
        "            one_type_in_1day = np.array(one_type_in_1day).reshape(24, int(672 / resize), int(512 / resize), 1)\n",
        "            \n",
        "            # セクションB #\n",
        "            if count == 0:\n",
        "                all_type_in_1day = one_type_in_1day\n",
        "            else:\n",
        "                all_type_in_1day = np.concatenate([all_type_in_1day, one_type_in_1day], axis=3)\n",
        "            # セクションB #\n",
        "                \n",
        "        dataset.append(all_type_in_1day)\n",
        "    \n",
        "    # セクションC #\n",
        "    dataset = np.array(dataset, dtype='float32').reshape(len(start_date_list), 24, int(672 / resize), int(512 / resize), len(data_type_list))\n",
        "    save_name = f'{phase}_met.npy'\n",
        "    np.save(save_name, dataset)\n",
        "    # セクションC #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ4VdWLXwjfZ"
      },
      "source": [
        "# [3] 前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnmhCsGRxJxx"
      },
      "source": [
        "## 1.学習データと検証データの分割"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7AwJLBnHMs-"
      },
      "source": [
        "### 1-1.学習データと検証データの分割"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRlFqQp0HTKR"
      },
      "source": [
        "**[2]で作成した4つのnpyファイル**\n",
        "```\n",
        "説明\tファイル名\t形状\n",
        "学習用 衛星画像データ\ttrain_sat.npy\t(730, 24, 33, 25, 1)\n",
        "テスト用 衛星画像データ\ttest_sat.npy\t(50, 24, 33, 25, 1)\n",
        "学習用 気象データ\ttrain_met.npy\t(730, 24, 33, 25, 34)\n",
        "テスト用 気象データ\ttest_met.npy\t(50, 24, 33, 25, 34)\n",
        "```\n",
        "**npyファイルの読み込み方**\n",
        "```\n",
        "# npyファイルの読み込み\n",
        "train_sat_dataset = np.load('train_sat.npy')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiBifhYSH3gH"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "\n",
        "# train_sat.npyを読み込む\n",
        "train_sat_dataset = np.load('train_sat.npy')\n",
        "\n",
        "# 学習用データを抽出する\n",
        "train_sat = train_sat_dataset[:365]\n",
        "\n",
        "# 検証用データを抽出する\n",
        "val_sat = train_sat_dataset[365:]\n",
        "\n",
        "\n",
        "print(train_sat.shape)\n",
        "print(val_sat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwwxzZYqxJ1a"
      },
      "source": [
        "## 2.データの正規化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjtiHBXH9bj"
      },
      "source": [
        "### 2-1.衛星画像データの正規化を行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy_AJ3F0IBzw"
      },
      "source": [
        "**正規化の方針**\n",
        "```\n",
        "# 全ての値を0以上1以下の間に収める手法\n",
        "\n",
        "# 衛星画像データを255で割る\n",
        "train_sat = train_sat / 255\n",
        "```\n",
        "**データ型を変更**\n",
        "```\n",
        "# uint8型のデータをfloat32型のデータに変換\n",
        "train_sat = train_sat.astype(np.float32)\n",
        "```\n",
        "**補足) float32型を採用する理由**\n",
        "```\n",
        "float32型とfloat64型は、\n",
        "1.   数値表現の精密さ\n",
        "2.   メモリサイズ\n",
        "のトレードオフ\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxeQxTVgI24T"
      },
      "source": [
        "# 衛星画像データを正規化する\n",
        "train_sat = train_sat.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "\n",
        "print(train_sat.dtype)\n",
        "print(train_sat.max())\n",
        "print(train_sat.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Rvf_NrJA2F"
      },
      "source": [
        "### 2-2.気象データの正規化を行う① 最大値、最小値の取得"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFotg4ucJFl1"
      },
      "source": [
        "**気象データを種類別に抽出する**\n",
        "```\n",
        "# 1種類目の気象データのみを抽出する\n",
        "train_met_0 = train_met[:, :, :, :, 0]\n",
        "\n",
        "\"\"\"\n",
        "形状が(730, 24, 33, 25, 34)であるtrain_metに対して、\n",
        "5つ目の次元でインデックス0を指定することで、1種類目の気象データのみが抽出され、\n",
        "train_met_0は、形状(730, 24, 33, 25)となる\n",
        "\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fQ8D555JV3x"
      },
      "source": [
        "# 気象データの最大値、最小値を取得する関数get_met_max_min()\n",
        "def get_met_max_min(train_met):\n",
        "    # 気象データのチャンネル数34を取得する\n",
        "    n_data_type = train_met.shape[4]\n",
        "    \n",
        "    max_array = []\n",
        "    min_array = []\n",
        "    \n",
        "    # 0から33までのインデックスを順番に取得する\n",
        "    for i in range(n_data_type):\n",
        "    \n",
        "        # チャンネルの次元をインデックスiで指定し、\n",
        "        # チャンネルごとの最大値、最小値を取得する\n",
        "        max_value = np.max(train_met[:, :, :, :, i])\n",
        "        min_value = np.min(train_met[:, :, :, :, i])\n",
        "        \n",
        "        max_array.append(max_value)\n",
        "        min_array.append(min_value)\n",
        "        \n",
        "    max_array = np.array(max_array)\n",
        "    min_array = np.array(min_array)\n",
        "    \n",
        "    return max_array, min_array\n",
        "\n",
        "\n",
        "# get_met_max_min()関数を実行する\n",
        "max_array, min_array = get_met_max_min(train_met=train_met)\n",
        "\n",
        "print('各種気象データの最大値を格納した配列', max_array)\n",
        "print('--------------------------------------------------------------')\n",
        "print('各種気象データの最小値を格納した配列', min_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZo70OwJa0n"
      },
      "source": [
        "### 2-3.気象データの正規化を行う② 正規化の実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cyGLFqkJhDw"
      },
      "source": [
        "**最大値、最小値を利用して、実際に各種の気象データの正規化**\n",
        "```\n",
        "normalize_met_data()関数\n",
        "\n",
        "引数として、\n",
        "met_data: 正規化前の気象データ\n",
        "max_array: 最大値を格納した配列\n",
        "min_array: 最小値を格納した配列\n",
        "を受け取り、返り値が正規化済みのNumPy配列となるように設計\n",
        "```\n",
        "**正規化の公式**\n",
        "```\n",
        "# 正規化の公式\n",
        "正規化後のデータ = (元のデータ - 最小値) / (最大値 - 最小値)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dr3Hf-9J5lO"
      },
      "source": [
        "# 気象データを[0, 1]の開区間に正規化する関数normalize_met_data\n",
        "def normalize_met_data(met_data, max_array, min_array):\n",
        "\n",
        "    n_data_type = met_data.shape[4]\n",
        "    \n",
        "    for i in range(n_data_type):\n",
        "        # max_array, min_arrayを利用して、正規化を実行する\n",
        "        met_data[:, :, :, :, i] = (met_data[:, :, :, :, i] - min_array[i]) / (max_array[i] - min_array[i])\n",
        "        \n",
        "    return met_data\n",
        "    \n",
        "\n",
        "train_met = normalize_met_data(met_data=train_met, \n",
        "                               max_array=max_array, min_array=min_array)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIAmsCBRxJ4m"
      },
      "source": [
        "## 3.入力データと正解データの分割"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eTf2dGUKFmm"
      },
      "source": [
        "### 3-1.入力データと正解データの分割"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wigNZtyEKQ3B"
      },
      "source": [
        "**rain_sat val_satの分割**\n",
        "```\n",
        "学習データと検証データの現在の形状\n",
        "*   train_sat: (365, 24, 33, 25, 1)\n",
        "*   val_sat: (365, 24, 33, 25, 1)\n",
        "\n",
        "インデックス指定でデータを抽出することで、同じインデックスが示すデータが、『入力』&『正解』の関係として成り立つように分割\n",
        "*   入力データのインデックス:0〜363\n",
        "*   正解データのインデックス:1〜364\n",
        "```\n",
        "**インデックス配列の作成**\n",
        "\n",
        "```\n",
        "*   入力:0〜363\n",
        "*   正解:1〜364\n",
        "```\n",
        "**np.arange()関数の引数**\n",
        "\n",
        "```\n",
        "*   第1引数: start\n",
        "*   第2引数: stop\n",
        "*   第3引数: step\n",
        "\n",
        "*   区間が[start, stop)\n",
        "*   公差(等差数列における隣の項との差)がstep\n",
        "\n",
        "# 0から10までの整数を並べる\n",
        "array = np.arange(0, 11, 1)\n",
        "\n",
        "np.arange()関数が返り値とする等差数列は、\n",
        "*   start以上(start <= x)\n",
        "*   stop未満(x < stop)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8zcBRJHLT7g"
      },
      "source": [
        "**補足) np.arange()関数の省略**\n",
        "```\n",
        "1.   位置引数を2つのみ指定した場合\n",
        "# 0から10までの整数を並べる\n",
        "array = np.arange(0, 11)\n",
        "\n",
        "\"\"\"\n",
        "位置引数は\n",
        "start = 0\n",
        "stop = 11\n",
        "を指定したと判断される\n",
        "\n",
        "stepはデフォルトの1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "2.   位置引数を1つのみ指定した場合\n",
        "# 0から10までの整数を並べる\n",
        "array = np.arange(11)\n",
        "\n",
        "\"\"\"\n",
        "位置引数は\n",
        "stop = 11\n",
        "を指定したと判断される\n",
        "\n",
        "startはデフォルトの0\n",
        "stepはデフォルトの1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzurqJB6Ln2k"
      },
      "source": [
        "# 入力データのインデックス配列をX_indexに代入する\n",
        "X_index = np.arange(364)\n",
        "\n",
        "# 正解データのインデックス配列をY_indexに代入する\n",
        "Y_index = np.arange(1, 365)\n",
        "\n",
        "\n",
        "# インデックスに従って、入力データ、正解データを抽出する\n",
        "X_train = train_sat[X_index]\n",
        "Y_train = train_sat[Y_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPJp0ErNwobp"
      },
      "source": [
        "# [4] モデリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0qN_dqxWAq"
      },
      "source": [
        "## 1.Convolutional LSTMモデルの構築"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI4Iq7AqBnc4"
      },
      "source": [
        "### 1-1.Convolutional LSTM層を実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "penhUj9ZBsGQ"
      },
      "source": [
        "**Convolutional LSTMとは**\n",
        "```\n",
        "*   CNN(Convolutional Neural Network) 画像データの処理\n",
        "*   LSTM(Long short-term memory) 時系列データの処理\n",
        "という2つのネットワークを組み合わせることによって生まれた応用的なネットワーク\n",
        "```\n",
        "**KerasにおけるConvolutional LSTMの実装方法**\n",
        "```\n",
        "Tensorflow.keras\n",
        "# 一般的なニューラルネットワーク層\n",
        "*   Conv2D\n",
        "*   Dense\n",
        "*   LSTM\n",
        "\n",
        "# Convolutional LSTM層\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "\n",
        "# モデルを初期化する\n",
        "model = Sequential()\n",
        "\n",
        "# ConvLSTM層の追加\n",
        "model.add(ConvLSTM2D())\n",
        "```\n",
        "**ConvLSTM2D層に指定できる引数**\n",
        "```\n",
        "引数名\t引数に代入する値\t引数の説明\n",
        "filters\t20\tConvLSTM2Dが出力するフィルタ(チャンネル)の数\n",
        "kernel_size\t(3, 3)\t畳み込みに使用するウィンドウのサイズ\n",
        "padding\t'same'\t畳み込み後、画像サイズを入力サイズに合わせるための補間を実行するか否かの指定\n",
        "activation\t'tanh'\tConvLSTM層の出力時に使用する活性化関数\n",
        "recurrent_activation\t'sigmoid'\t各再帰(LSTM)セルの出力時に使用する活性化関数\n",
        "input_shape\t(24, 33, 25, 1)\tバッチサイズを除いた入力の形状。入力層のみ指定することが必須となっている\n",
        "return_sequences\tTrue\tTrueならば、出力された配列をそのまま返す。Falseならば、時系列方向で最後に出力された配列のみ返す。\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwZ9LsIDKoe"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "\n",
        "# Sequentialのインスタンスを生成することで、モデルの雛形とする\n",
        "model = Sequential()\n",
        "\n",
        "# ConvLSTM2D層を実装する\n",
        "model.add(ConvLSTM2D(filters=20, kernel_size=(3, 3), padding='same', return_sequences=True,\n",
        "                                            activation='tanh', recurrent_activation='sigmoid',\n",
        "                                            input_shape=(24, 33, 25, 1)))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IN0ysNPDcuR"
      },
      "source": [
        "### 1-2.Encoder、Decoderを実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoNwBlgZDjrv"
      },
      "source": [
        "**seq2seqを導入する目的**\n",
        "```\n",
        "時系列データの生成を可能にするため\n",
        "\n",
        "例えば、本来1時間後までしか予測することのできないモデルを、一度に24時間後まで予測できるようにする\n",
        "```\n",
        "**seq2seqの構成要素: EncoderとDecoder**\n",
        "```\n",
        "*   Encoder: 入力データの情報を圧縮する\n",
        "*   Decoder:Encoderによって圧縮されたデータを展開し、出力とする\n",
        "```\n",
        "**Encoder、Decoderの実装上の違い: 「return_sequences」**\n",
        "```\n",
        "*   Trueを指定すると、時系列方向について、全ての配列が出力となる\n",
        "*   Falseを指定すると、時系列方向について、最後の時点の配列のみが出力となる\n",
        "```\n",
        "**Kerasにおいて実装する場合**\n",
        "```\n",
        "*   Encoder層はreturn_sequences=False\n",
        "*   Decoder層はreturn_sequences=True\n",
        "```\n",
        "**Encoderにおいて、return_sequencesをFalseとする理由**\n",
        "```\n",
        "RNN系(再帰型)のネットワークは、過去(t-1)の出力データを次の時点(t)の入力データになんらかの方法で受け渡すという工程を繰り返すことで、過去の状態を考慮した予測が可能になるというアルゴリズム。\n",
        "\n",
        "つまり、入力データの時系列方向のサイズをTとすると、T番目の出力(時系列方向で最後となる出力)は、1, 2, 3, .... , T- 2, T-1までの過去の情報を全て保持していると考えることができる。\n",
        "よって、入力データの圧縮が目的であるEncoder層は、過去の状態を全て保持したT番目の出力(時系列方向で最後の出力)のみを必要とし、1, 2, 3, ...., T-2, T-1の情報は出力から除外する。\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PzrL3r4v9rp"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "\n",
        "# Encoder層を実装する\n",
        "Encoder = Sequential()\n",
        "Encoder.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3), padding='same', return_sequences=False,\n",
        "                    activation='tanh', recurrent_activation='sigmoid',\n",
        "                    input_shape=(24, 33, 25, 1)))\n",
        "\n",
        "# Decoder層を実装する\n",
        "Decoder = Sequential()\n",
        "Decoder.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3), padding='same', return_sequences=True,\n",
        "                    activation='tanh', recurrent_activation='sigmoid',\n",
        "                    input_shape=(24, 33, 25, 20)))\n",
        "\n",
        "\n",
        "print(Encoder.summary())\n",
        "print(Decoder.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxeKdCZzwD-R"
      },
      "source": [
        "### 1-3.EncoderとDecoderをつなぐ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvAk6iIIwHfo"
      },
      "source": [
        "**現状**\n",
        "\n",
        "```\n",
        "Encoder入力前: (バッチサイズ, 24, 高さ, 幅, チャンネル数)\n",
        "Encoder出力後: (バッチサイズ, 高さ, 幅, チャンネル数)\n",
        "\n",
        "# 入力データが5次元の配列であることを必要とするDecoderのConvLSTM2Dに入力しようとするとエラーが発生してしまう\n",
        "\n",
        "\n",
        "1.   時系列方向の次元を復活させる\n",
        "(バッチサイズ, 高さ, 幅, チャンネル数) -> (バッチサイズ, 1, 高さ, 幅, チャンネル数)\n",
        "2.   配列をコピーし、時系列方向に連結することで、予測したい時系列の長さ(当クエストの場合、24)に揃える\n",
        "(バッチサイズ, 1, 高さ, 幅, チャンネル数) -> (バッチサイズ, 24, 高さ, 幅, チャンネル数)\n",
        "```\n",
        "**モデル内で任意の処理を実行する: Lambda層**\n",
        "\n",
        "```\n",
        "# Lambda層のインポート\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# Lambda層の追加\n",
        "model.add(Lambda(repeat_last_status))\n",
        "```\n",
        "**Lambdaの引数となる関数repeat_last_status**\n",
        "```\n",
        "*   引数として、前の層から出力されたTensorを受け取り\n",
        "*   返り値として、次の層に入力するTensorを設定する\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FrvajmYxLRJ"
      },
      "source": [
        "**tf.Tensor型**\n",
        "\n",
        "```\n",
        "# Tensorflowライブラリのインポート\n",
        "import tensorflow as tf\n",
        "```\n",
        "**1. 時系列方向の次元を復活させる(tf.reshape)**\n",
        "\n",
        "```\n",
        "# tf.reshape()関数による形状の変更\n",
        "x = tf.reshape(x, (-1, 1, 33, 25, 20))\n",
        "# -1と指定した次元の成分数は、他の次元に指定した値から推測され、自動的に決まる\n",
        "```\n",
        "**2. 配列をコピーし(tf.identity)、時系列方向に連結することで(tf.concat)、予測したい時系列の長さに揃える**\n",
        "\n",
        "```\n",
        "# tf.Tensor型のデータをコピーし、新しいオブジェクトとして生成する\n",
        "copied_x = tf.identity(x)\n",
        "\n",
        "\n",
        "# 時系列方向の次元(2つ目の次元)で配列を連結する\n",
        "x = tf.concat([x, copied_x], axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visanxn5x6Ff"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "def repeat_last_status(x):\n",
        "    \n",
        "    # 1. 時系列方向の次元を復活させる\n",
        "    x = tf.reshape(x, (-1, 1, 33, 25, 20))\n",
        "    \n",
        "    # tf.Tensor型のデータをコピーし、新しいオブジェクトとして生成する\n",
        "    copied_x = tf.identity(x)\n",
        "    \n",
        "    # 時系列方向を軸とした連結を23回繰り返す\n",
        "    for _ in range(23):\n",
        "        \n",
        "        x = tf.concat([x, copied_x], axis=1)\n",
        "        \n",
        "    return x\n",
        "\n",
        "# Sequentialのインスタンスを生成することで、モデルの雛形とする\n",
        "model = Sequential()\n",
        "# Lambda層を実装する\n",
        "model.add(Lambda(repeat_last_status))\n",
        "\n",
        "\n",
        "# 次元数が4の配列xを生成\n",
        "x = tf.ones((10, 33, 25, 20))\n",
        "\n",
        "# repeat_last_statusを実行する\n",
        "repeated_x = repeat_last_status(x)\n",
        "\n",
        "print('次元数が4の配列x:')\n",
        "print(x.shape)\n",
        "print('-----------------------------------------------')\n",
        "print('repeat_last_statusによって変換後の配列x:')\n",
        "print(repeated_x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiSFriWUyCxQ"
      },
      "source": [
        "### 1-4.出力層を実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZqeqmAyKqZ"
      },
      "source": [
        "**Decoderによって出力された配列に対して、「1x1サイズのカーネル」によって、畳み込みを行う層を追加**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wMvDj5yMCk"
      },
      "source": [
        "**TimeDistributed**\n",
        "\n",
        "```\n",
        "配列を時系列の軸で分割し、分割後の配列それぞれに対して、演算を行う\n",
        "\n",
        "# TimeDistributedのインポート\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv2D\n",
        "\n",
        "# TimeDistributedを用いた層の追加\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')))\n",
        "```\n",
        "**出力層の演算: Conv2Dについて**\n",
        "\n",
        "```\n",
        "1. filters(フィルター数)\n",
        "Conv2Dのフィルター数(filtes)は1設定することで、出力される配列の形状がY(正解データ)と同じく、(バッチサイズ, 高さ, 幅, 1)となるようにする\n",
        "\n",
        "2. activation(活性化関数)\n",
        "Conv2Dのactivation(活性化関数)は、シグモイド関数(sigmoid)を選択する\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Conv2Dの引数名\t引数に代入する値\n",
        "filters\t1\n",
        "kernel_size\t(1, 1)\n",
        "activation\t'sigmoid'\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ewzGMYy9VC"
      },
      "source": [
        "# ライブラリのインポート\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv2D\n",
        "\n",
        "# Sequentialのインスタンスを生成することで、モデルの雛形とする\n",
        "model = Sequential()\n",
        "\n",
        "# TimeDistributedでラップされたConv2Dを実装する\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid'),\n",
        "                         input_shape=(24, 33, 25, 20)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mqL45cFzDGU"
      },
      "source": [
        "### 1-5.損失関数と最適化手法を設定する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBTgsmchzKqq"
      },
      "source": [
        "**compileメソッド**\n",
        "\n",
        "```\n",
        "\"\"\"例)\n",
        "損失関数に交差エントロピー\n",
        "最適化手法に確率的勾配降下法を指定する場合\n",
        "\"\"\"\n",
        "\n",
        "# 損失関数、最適化手法の指定\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "```\n",
        "**1. 損失関数の選択**\n",
        "```\n",
        "MAE(平均絶対誤差: Mean Absolute Error)を採用する\n",
        "```\n",
        "**2. 最適化手法の選択**\n",
        "\n",
        "```\n",
        "Adamを選択する\n",
        "\n",
        "Adamは、\n",
        "*   学習率: learning_rate\n",
        "*   モーメンタム: momentum\n",
        "といった最適化手法に関するパラメータを特にいじることなく、デフォルトの設定のままで学習を実行しても一定の成功が保証されている使い勝手の良い最適化手法\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpf4uXS5z4mP"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Encoder\n",
        "model.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3), \n",
        "                    padding='same', return_sequences=False,\n",
        "                    activation='tanh', recurrent_activation='sigmoid',\n",
        "                    input_shape=(24, 33, 25, 1)))\n",
        "\n",
        "# EncoderとDecoderをつなぐ中間部分\n",
        "def repeat_last_status(x):\n",
        "    x = tf.reshape(x, (-1, 1, 33, 25, 20))\n",
        "    copied_x = tf.identity(x)\n",
        "    for _ in range(23):\n",
        "        x = tf.concat([x, copied_x], axis=1)        \n",
        "    return x\n",
        "model.add(Lambda(repeat_last_status))\n",
        "\n",
        "# Decoder\n",
        "model.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3),\n",
        "                    padding='same', return_sequences=True,\n",
        "                    activation='tanh', recurrent_activation='sigmoid'))          \n",
        "\n",
        "# 出力層\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')))\n",
        "          \n",
        "\n",
        "# モデルをコンパイルする\n",
        "model.compile(loss='mae', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwQVVIwrxWFd"
      },
      "source": [
        "## 2.コールバックの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg8fGUzC0ENC"
      },
      "source": [
        "### 2-1.コールバックとは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3V1YiiH0OGn"
      },
      "source": [
        "```\n",
        "コールバックは、主にモデルの学習の進行状況に応じて、特定の処理を行うことのできる機能の集まり\n",
        "\n",
        "使用頻度の高いものとして、\n",
        "*   EarlyStopping: 評価スコアを監視し、モデルの改善が止まったタイミングで学習の早期終了を行う\n",
        "*   ModelCheckpoint: 各エポック終了時のモデルの状態を保存する\n",
        "*   Tensorboard: 可視化ツール「Tensorboard」の使用に必要なログを保存する\n",
        "などがある。\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBHQtHUW0n4E"
      },
      "source": [
        "### 2-1.EarlyStoppingを実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKQ701pl0voC"
      },
      "source": [
        "**1. EarlyStoppingのインポート**\n",
        "\n",
        "```\n",
        "# EarlyStoppingのインポート\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "```\n",
        "**2. EarlyStoppingのインスタンス生成**\n",
        "\n",
        "```\n",
        "EarlyStopping(早期終了)を実行するには、どういう基準で学習を早期に終わらせるのかという点を明らかにする必要\n",
        "\n",
        "・ EarlyStoppingのインスタンスを生成する際の引数\n",
        "引数名\t引数の説明\n",
        "monitor\tモデルの学習が上手くいっているのか否かの判定に使用する評価基準の指定\n",
        "patience\tmonitorに設定した評価スコアの改善が止まって以降何エポック後に学習を終了させるかの指定\n",
        "\n",
        "# monitorに'val_loss', patienceに30を指定してインスタンス生成\n",
        "e_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRmzxY8D1PCa"
      },
      "source": [
        "# EarlyStoppingのインポート\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# EarlyStoppingのインスタンスを生成する\n",
        "e_stopping = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZACyGZ111WLT"
      },
      "source": [
        "### 2-3.ModelCheckpointを実装する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQxpRAhe1abj"
      },
      "source": [
        "\n",
        "```\n",
        "ModelCheckpointは、各エポック終了時のモデルの状態をファイルとして保存する役割\n",
        "```\n",
        "**1. ModelCheckpointのインポート**\n",
        "```\n",
        "# ModelCheckpointのインポート\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "```\n",
        "**2. ModelCheckpointのインスタンス生成**\n",
        "```\n",
        "ModelCheckpointのインスタンス生成の際の引数\n",
        "引数名\t引数の説明\n",
        "filepath\tモデルの状態を保存するファイル名の指定\n",
        "monitor\tモデルの学習が上手くいっているのか否かの判定に使用する評価基準\n",
        "save_best_only\tTrueとした場合、モデルのmonitorの値がベストスコアを更新した場合のみモデルの保存を実行する\n",
        "save_weights_only\tTrueとした場合、パラメータのみを保存の対象とし、モデルの構造などその他の状態の保存は行わない\n",
        "\n",
        "# ModelCheckpointクラスのインスタンス生成\n",
        "checkpoint = ModelCheckpoint(filepath='my_params.h5',\n",
        "                             monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "```\n",
        "**filepathの柔軟な変更**\n",
        "```\n",
        "置換フィールド{}に指定することのできる主な変数名\n",
        "変数名\t変数の説明\n",
        "epoch\t第何エポック終了時点であるのかを示す整数\n",
        "loss\tエポック終了時点における学習データに対する損失値\n",
        "val_loss\tエポック終了時点における検証データに対する損失値\n",
        "\n",
        "# ModelCheckpointクラスのインスタンス生成\n",
        "checkpoint = ModelCheckpoint(filepath='my_params_Epoch{epoch}_loss{loss:.4f}_valloss{val_loss:.4f}.h5',\n",
        "                             monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksw2WS_E2I_B"
      },
      "source": [
        "# ModelCheckpointのインポート\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# ModelCheckpointのインスタンスを生成する\n",
        "checkpoint = ModelCheckpoint(filepath='my_model_Epoch{epoch}_loss{loss:.4f}_valloss{val_loss:.4f}.h5',\n",
        "                             monitor='val_loss', save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3sxV2C9xWLX"
      },
      "source": [
        "## 3.モデルの学習と推論の実行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB1dIwqx2T4Q"
      },
      "source": [
        "### 3-1.モデルの学習を実行する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfopcdNT2ZfU"
      },
      "source": [
        "```\n",
        "# Sequentialクラスのインスタンス生成\n",
        "model = Sequential()\n",
        "```\n",
        "**model.fit()メソッドに指定する引数**\n",
        "```\n",
        "引数名\tデータ型\t引数の説明\n",
        "x\tNumPy配列\t学習に使用する入力データ\n",
        "y\tNumPy配列\t学習に使用する正解データ\n",
        "batch_size\t整数\tバッチサイズ\n",
        "epochs\t整数\tエポック数\n",
        "validation_data\tタプル\t検証に使用する入力データと正解データのペア\n",
        "callbacks\tリスト\tコールバック\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_ANJVsO2qMK"
      },
      "source": [
        "# 検証用データをタプルにまとめる\n",
        "validation_data = (X_val, Y_val)\n",
        "\n",
        "# EarlyStoppingの設定\n",
        "e_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "# Checkpointの設定\n",
        "checkpoint = ModelCheckpoint(filepath='my_model_Epoch{epoch}_loss{loss:.4f}_valloss{val_loss:.4f}.h5',\n",
        "                             monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "                             \n",
        "# callbacksをリストにまとめる\n",
        "callbacks = [e_stopping, checkpoint]\n",
        "\n",
        "# 学習を実行する\n",
        "model.fit(x=X_train, y=Y_train, batch_size=16, epochs=1,\n",
        "                 validation_data=validation_data, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q5GStDx24Yd"
      },
      "source": [
        "### 3-2.モデルの学習がどのように進行したのかを確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3UIyj627ub"
      },
      "source": [
        "**各エポックにおけるMAE評価の推移**\n",
        "```\n",
        "グラフから読み取ることのできる情報\n",
        "\n",
        "1.   MAE評価の推移\n",
        "学習開始後、しばらくは、loss(学習時のMAE)、val_loss(検証時のMAE)ともに順調に改善を続けている\n",
        "2.   EarlyStoppingの動作\n",
        "エポック100あたりでEarlyStoppingが作動し、学習が終了している\n",
        "3.   未知のデータに対するMAE評価のベストスコア\n",
        "MAE評価0.085付近で微増減を繰り返していることから、作成したモデルは、MAE評価において誤差約0.085程度の予測精度をもつモデルである\n",
        "```\n",
        "**誤差のスケール**\n",
        "```\n",
        "衛星画像データを構成する値を255分の1とする正規化処理を行ったことため、誤差のスケールも同じく255分の1となっている\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGr3aWMV38PN"
      },
      "source": [
        "### 3-3.学習済みのモデルを使って推論を行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOzdIsfB4BW-"
      },
      "source": [
        "**1. パラメータが保存されたHDF5ファイルをモデルに読み込む**\n",
        "```\n",
        "# パラメータ情報の読み込み\n",
        "model.load_weights('my_params.h5')\n",
        "```\n",
        "**2. 推論の実行**\n",
        "```\n",
        "# テスト用データについての推論\n",
        "Y_pred = model.predict(X_test)\n",
        "```\n",
        "**「モデルが生成する予測画像」と「正解画像」の違いについて**\n",
        "```\n",
        "*   「台風」の動きを予測できているか\n",
        "*   「梅雨前線」の動きを予測できているか\n",
        "*   季節ごとに、どのような予測精度の違いが存在するのか\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LslqJhh-4hpe"
      },
      "source": [
        "# 学習済みのパラメータを読み込む\n",
        "model.load_weights('my_params.h5')\n",
        "\n",
        "# 推論を実行し、テスト用データの予測値を生成する\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt5hpoKowoes"
      },
      "source": [
        "# [5] 予測精度の改善"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zREDngADxjmC"
      },
      "source": [
        "## 1.特徴量として気象データを追加"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-U9yexO5Okm"
      },
      "source": [
        "### 1-1.気象データを特徴量として加える"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd9WV5dT5Ued"
      },
      "source": [
        "```\n",
        "果たして、衛星画像の未来予測において気象データが特徴量として有効に機能するのかという点に注目し、改めてモデルの学習を実行\n",
        "```\n",
        "**気象データを特徴量として追加する方法**\n",
        "\n",
        "```\n",
        "チャンネルの次元で連結することで特徴量の追加\n",
        "np.concatenate()関数\n",
        "\n",
        "*   第1引数: 連結の対象とする配列、\n",
        "*   axis: 連結部分となる次元のインデックス\n",
        "\n",
        "# 衛星画像データと気象データをチャンネルの次元で連結する\n",
        "X_train = np.concatenate([X_train_sat, X_train_met], axis=4)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkxDEHuW539N"
      },
      "source": [
        "X_train = np.concatenate([X_train_sat, X_train_met], axis=4)\n",
        "\n",
        "print('X_train_satの形状:')\n",
        "print(X_train_sat.shape)\n",
        "print('----------------------')\n",
        "print('X_train_metの形状:')\n",
        "print(X_train_met.shape)\n",
        "print('----------------------')\n",
        "print('X_trainの形状:')\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml1oUywB59A0"
      },
      "source": [
        "### 1-2.モデルに変更を加える"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcYCyKR66C0R"
      },
      "source": [
        "```\n",
        "モデルの入力層の引数input_shapeのチャンネルの次元を衛星画像データのチャンネル数=1から衛星画像データ+気象データのチャンネル数=35に書き換える\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btRg0vGp6KIv"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Encoder\n",
        "model.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3), padding='same', return_sequences=False,\n",
        "                    activation='tanh', recurrent_activation='sigmoid',\n",
        "                    input_shape=(24, 33, 25, 35)))\n",
        "\n",
        "# EncoderとDecoderをつなぐ中間部分\n",
        "def repeat_last_status(x):\n",
        "    x = tf.reshape(x, (-1, 1, 33, 25, 20))\n",
        "    copied_x = tf.identity(x)\n",
        "    for _ in range(23):\n",
        "        x = tf.concat([x, copied_x], axis=1)        \n",
        "    return x\n",
        "model.add(Lambda(repeat_last_status))\n",
        "\n",
        "# Decoder\n",
        "model.add(ConvLSTM2D(\n",
        "                    filters=20, kernel_size=(3, 3),\n",
        "                    padding='same', return_sequences=True,\n",
        "                    activation='tanh',\n",
        "                    recurrent_activation='sigmoid'))          \n",
        "\n",
        "# 出力層\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbWTo8ps6VxK"
      },
      "source": [
        "### 1-3.気象データを加えた上で再度学習を行った結果を確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cbr6Axv6dtj"
      },
      "source": [
        "```\n",
        "気象データは特徴量として有効に機能する\n",
        "```"
      ]
    }
  ]
}