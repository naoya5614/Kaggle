{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ubiquant_Market_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7yqU0482PaGMxN0ka95yQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naoya5614/SIGNATE-Kaggle/blob/main/Ubiquant_Market_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ubiquauantコンペアイデア"
      ],
      "metadata": {
        "id": "-vLPrTrJAAPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9PsUTJq_5Sq"
      },
      "outputs": [],
      "source": [
        "import gc, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import log_loss, mean_squared_error\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (16, 5)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rain = (pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\n",
        ".sort_values(['time_id', 'investment_id'])\n",
        ".drop(columns=['row_id'])\n",
        ".query('time_id > 599')\n",
        ".reset_index(drop=True));\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "-4Ku60K7FGR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_columns = train.colomuns\n",
        "features = all_columns[train.columns.str.contains('f_')]"
      ],
      "metadata": {
        "id": "Hh8xGpVCF6a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "CaQvTfijGEy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "W0WfWbLnGHX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn,model_selection import KFold\n",
        "from sklearn.model_selection._split import _BaseKFold, indexable, _num_sample\n",
        "from sklearn.untils.validation import _depcate_positional_args\n",
        "\n",
        "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
        "  @_deprecate_positional_args\n",
        "  def __init__(self,\n",
        "               n_splits=5,\n",
        "               *,\n",
        "               max_train_group_size=np.inf,\n",
        "               max_test_group_size=np.inf,\n",
        "               group_gap=None,\n",
        "               varbose=False\n",
        "               ):\n",
        "    super().__init__(n_splits, shuffle=False, random_state=None)\n",
        "    self.max_train_group_size = max_train_group_size\n",
        "    self.verbose = verbose\n",
        "    \n",
        "    def split(self, X, y=None, groups=None):\n",
        "      if groups is None\n",
        "      raise ValueError(\n",
        "          X, y, groups = indexable(X, y, groups)\n",
        "          n_samples = num_samples(X)\n",
        "          n_splits = self.n_splits\n",
        "          group_gap = self.group_gap\n",
        "          max_test_group_size = self.max_test_group_size\n",
        "          max_train_group_size = self.max_train_group_size\n",
        "          n_folds = n_splits + 1\n",
        "          group_dict = {}\n",
        "          u, ind = np.unique(groups, return_index=True)\n",
        "          unique_groups = u[np.argsort(ind)]\n",
        "          n_samples = _num_samples(X)\n",
        "          n_groups = _num_samples(unique_groups)\n",
        "          for idx in np.arange(n_samples):\n",
        "            if (groups[idx] in group_dict):\n",
        "              group_dict[groups[idx]].append(idx)\n",
        "            else:\n",
        "              group_dict[groups[idx]] = [idx]\n",
        "          if n_folds > n_groups:\n",
        "            raise ValueError(\n",
        "                (\"Cannot have number of folds={0} greater than\"\n",
        "                \"the number of groups={1}\").format(n_folds,\n",
        "                                                   n_groups))\n",
        "            group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
        "            group_test_starts = range(n_groups - n_splits * group_test_size,\n",
        "                                      \n",
        "                                      n_groups, group_test_size)\n",
        "            for group_test_start in group_test_starts:\n",
        "              train_array = []\n",
        "              test_array = []\n",
        "\n",
        "              group_st = max(0, group_test_start - froup_gap - max_train_group_size)\n",
        "              for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
        "                train_array_tmp = group_dict[train_group_idx]\n",
        "                train_array = np.sort(np.unique(np.concatnate((train_array,\n",
        "                                                               train_array_tmp)),\n",
        "                                                \n",
        "                                                axis=None), axis=None)\n",
        "                train_end = train_array.size\n",
        "\n",
        "                for test_group_idx in unique_groups[group_test_start:\n",
        "                                                    group_test_start + group_test_size]:\n",
        "                       test_array_tmp = group_dict[test_group_idx]\n",
        "                       test_array = np.sort(np.unique(np.concatenate((test_array, test_array_tmp)),\n",
        "                                                      axis=None), axis=None)\n",
        "                       test_array = test_array[group_gap:]\n",
        "\n",
        "                       if self.verbose > 0:\n",
        "                         pass\n",
        "                       yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
        "                       \n",
        "\n",
        "\n",
        "      ))"
      ],
      "metadata": {
        "id": "kflbnMDHu36i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}