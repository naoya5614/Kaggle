{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIGNATE_Study2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYu3JDvWLTIwIySQJtBKUe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etG3qMrco4mM"
      },
      "source": [
        "## **Q3. 鋳造製品の欠陥検出**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGqGSIOEpCr-"
      },
      "source": [
        "NO\tライブラリ名\t用途\n",
        "1. numpy\t数値計算を行うため\n",
        "2. PIL\t画像処理を行うため\n",
        "3. os\tファイルの場所を特定するため\n",
        "4. matplotlib\t画像を図としてプロットするため\n",
        "5. torch\t画像認識モデルを作成するため\n",
        "6. torchvision\tデータセットを作成するため"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwvxLfILlWep"
      },
      "source": [
        "<p>### **データ分析の流れ・よく使うコマンド**\n",
        "<p>①画像の読み込み、欠陥の確認\n",
        "<p>1. 画像の読み込み\n",
        "<p>from PIL import Image\n",
        "<p>im = Image.open('画像ファイル')\n",
        "<p>type()\n",
        "<p>.format\n",
        "<p>.size\n",
        "<p>.mode\n",
        "<p>import matplotlib.pyplot as plt\n",
        "<p>plt.imshow(変数名)\n",
        "<p>import matplotlib.pyplot as plt\n",
        "<p>from PIL import Image\n",
        "<p>変数名 = Image.open('画像ファイル')\n",
        "<p>plt.imshow(変数名)\n",
        "<p>plt.show()\n",
        "<p>subplot(縦に並べるプロットの数, 横に並べるプロットの数, プロット番号(何番目のプロットか))\n",
        "<p>plt.subplot(1,3,1)\n",
        "<p>plt.text(0.5,0.5,\"1\")\n",
        "<p>※1行3列の2番目\n",
        "<p>plt.subplot(1,3,2)\n",
        "<p>plt.text(0.5,0.5,\"2\")\n",
        "<p>※1行3列の3番目\n",
        "<p>plt.subplot(1,3,3)\n",
        "<p>plt.text(0.5,0.5,\"3\")\n",
        "<p>plt.tight_layout()\n",
        "<p>from PIL import Image, ImageDraw\n",
        "<p>Image.new(画像タイプ, 画像の大きさ（tuple型）, 塗りつぶしの色)\n",
        "<p>変数 = ImageDraw.Draw(変数)\n",
        "<p>rectangle([(矩形左上X座標 , Y座標) , (矩形右下X座標 , Y座標)], <p>outline='アウトラインの色', width=大きさ)\n",
        "<p>ellipse([(図形左上X座標 , Y座標) , (図形右下X座標 , Y座標)], outline='アウトラインの色', width=大きさ)\n",
        "<p>\n",
        "<p>2. 画像の特徴理解\n",
        "<p>画像変数.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "<p>画像変数.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "<p>画像変数.point(lambda x: x * 1.5)\n",
        "<p>画像変数.resize((60, 60)).resize((300, 300))\n",
        "<p>画像変数.filter(ImageFilter.GaussianBlur(4))\n",
        "<p>from PIL import Image,ImageFilter\n",
        "<p>画像変数.filter(ImageFilter.FIND_EDGES)\n",
        "<p>\n",
        "<p>\n",
        "<p>---\n",
        "<p>\n",
        "<p>\n",
        "<p>②画像分類モデル作成準備\n",
        "<p>1. 集めた画像データの確認\n",
        "<p>import os\n",
        "<p>print(os.listdir('./'))\n",
        "<p>os.path.getsize('サイズを確認したいファイル')\n",
        "<p>import zipfile\n",
        "<p>with zipfile.ZipFile('zipファイル') as existing_zip:\n",
        "<p>            print(existing_zip.namelist()[0:5])\n",
        "<p>※zip解凍用の関数定義\n",
        "<p>def unzip_dataset(INPATH,OUTPATH):\n",
        "<p>    with zipfile.ZipFile(INPATH) as zf:\n",
        "<p>        zf.extractall(OUTPATH)\n",
        "<p>※zip解凍(※数秒から数十秒かかる場合があります)\n",
        "<p>unzip_dataset(INPATH='解凍したいzipのパス',OUTPATH='解凍を行いたい場所のパス')\n",
        "<p>.extractall()\n",
        "<p>len(ファイル数を調べたいファイル)\n",
        "<p>len(set(一意な値の数だけ調べたいファイル))\n",
        "<p>\n",
        "<p>\n",
        "<p>---\n",
        "<p>\n",
        "<p>\n",
        "<p>2. データセット作成\n",
        "<p>【transforms/Dataset/DataLoaderの役割】\n",
        "<p>  ※データセット作成の流れはtransforms→Dataset→DataLoader\n",
        "<p>    1. transforms\n",
        "<p>    ・データ前処理を定義するモジュール\n",
        "<p>    2. Dataset\n",
        "<p>    ・データと対応するラベルを1組返すモジュール\n",
        "<p>    ・transformsを使って定義した前処理を画像データに対して行うモジュール\n",
        "<p>    3. DataLoader\n",
        "<p>    ・データセットからデータをバッチサイズに固めて返すモジュール\n",
        "<p>\n",
        "<p>一般的な前処理\n",
        "<p>    1. データ前処理\n",
        "<p>    ・画像サイズを256に統一（(訓練データ、検証データともに）\n",
        "<p>    ・テンソル化(訓練データ、検証データともに）\n",
        "<p>    ・正規化(訓練データ、検証データともに)\n",
        "<p>    2. Data Augmentation\n",
        "<p>    ・水平方向へのランダム反転(※訓練データに対してのみ)\n",
        "<p>\n",
        "<p>from torchvision import transforms\n",
        "<p>data_transforms = {\n",
        "<p>    'train': transforms.Compose([\n",
        "<p>        学習時に行うデータ処理１,\n",
        "<p>        学習時に行うデータ処理2,\n",
        "<p>    ]),\n",
        "<p>    'val': transforms.Compose([\n",
        "<p>        検証時に行うデータ処理1,\n",
        "<p>        検証時に行うデータ処理2,\n",
        "<p>    ]),\n",
        "<p>}\n",
        "<p>transforms.ToTensor()\n",
        "<p>data_transforms = {\n",
        "<p>    'train': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.ToTensor(),     \n",
        "<p>    ]),\n",
        "<p>    'val': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.ToTensor(),\n",
        "<p>\n",
        "<p>    ]),\n",
        "<p>}\n",
        "<p>transforms.RandomHorizontalFlip()\n",
        "<p>data_transforms = {\n",
        "<p>    'train': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.RandomHorizontalFlip(p=0.5),\n",
        "<p>        transforms.ToTensor(), \n",
        "<p>    ]),\n",
        "<p>    'val': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.ToTensor(), \n",
        "<p>    ]),\n",
        "<p>}\n",
        "<p>transforms.Normalize()\n",
        "<p>※Imagenetと呼ばれる画像データセットの平均と標準偏差を使用するのは一般的な方法です。そのため、平均に[0.485, 0.456, 0.406]、標準偏差に[0.229, 0.224, 0.225]を指定\n",
        "<p>data_transforms = {\n",
        "<p>    'train': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.RandomHorizontalFlip(p=0.5),\n",
        "<p>        transforms.ToTensor(),\n",
        "<p>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "<p>    ]),\n",
        "<p>    'val': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.ToTensor(),\n",
        "<p>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "<p>    ]),\n",
        "<p>}\n",
        "<p>datasets.ImageFolder()\n",
        "<p>※各フォルダに学習用と検証用のデータが揃い、かつ、クラスごと(=ラベルごと)にフォルダが分かれている場合\n",
        "<p>from torchvision import transforms,datasets\n",
        "<p>※transformsの定義\n",
        "<p>data_transforms = {\n",
        "<p>    'train': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.RandomHorizontalFlip(p=0.5),\n",
        "<p>        transforms.ToTensor(),\n",
        "<p>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "<p>    ]),\n",
        "<p>    'val': transforms.Compose([\n",
        "<p>        transforms.Resize(256),\n",
        "<p>        transforms.ToTensor(),\n",
        "<p>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "<p>    ]),\n",
        "<p>}\n",
        "<p>※datasetsの定義\n",
        "<p>image_datasets = {\n",
        "<p>    'train': datasets.ImageFolder('./image_data/train',<p>data_transforms['train']),\n",
        "<p>    'val': datasets.ImageFolder('./image_data/val',<p>data_transforms['val'])\n",
        "<p>}\n",
        "<p>print(image_datasets['train'].samples[0:5])\n",
        "<p>print(image_datasets['train'].class_to_idx)\n",
        "<p>torch.utils.data.DataLoader(下記引数)\n",
        "<p>    ・引数dataset: loaderで読み込むデータセット\n",
        "<p>    ・引数batch_size: バッチサイズ\n",
        "<p>    ・引数shuffle: データをshuffleするか（True もしくは False）\n",
        "<p>    ・引数num_workers: 実行プロセス数（デフォルトは0）\n",
        "<p>    ・引数drop_last: 残りデータ数がバッチサイズに満たない場合にスキップするか（True もしくは False）\n",
        "<p>import torch\n",
        "<p>image_dataloaders = {\n",
        "<p>    'train': torch.utils.data.DataLoader(image_datasets<p>['train'], batch_size=4,shuffle=True, num_workers=0, <p>drop_last=True),\n",
        "<p>    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=4,shuffle=False, num_workers=0, drop_last=True),\n",
        "<p>}\n",
        "<p>for i ,(inputs,labels) in enumerate(image_dataloaders['train']):\n",
        "<p>            print(inputs)\n",
        "<p>            print(labels)\n",
        "<p>            if i == 0:\n",
        "<p>                break\n",
        "<p>\n",
        "<p>\n",
        "<p>---\n",
        "<p>\n",
        "<p>\n",
        "<p>③欠陥検出モデルの作成【転移学習無】\n",
        "<p>1. モデルの選定/作成\n",
        "<p>【torchvisionに実装済みの代表的画像分類モデル】\n",
        "<p>    ・AlexNet\n",
        "<p>    ・VGG\n",
        "<p>    ・ResNet\n",
        "<p>    ・SqueezeNet\n",
        "<p>    ・DenseNet\n",
        "<p>    ・Inception v3\n",
        "<p>    ・GoogLeNet\n",
        "<p>    ・ShuffleNet v2\n",
        "<p>    ・MobileNet v2\n",
        "<p>    ・ResNeXt\n",
        "<p>    ・Wide ResNet\n",
        "<p>    ・MNASNet\n",
        "<p>    ・etc...\n",
        "<p>from torchvision import datasets, models, transforms\n",
        "<p>model_ft = models.resnet18(pretrained=False)\n",
        "<p>import torch.nn as nn\n",
        "<p>model_ft.fc = nn.Linear(in_features=512, out_features=2, <p>bias=True)\n",
        "<p>import torch.nn as nn\n",
        "<p>from torchvision import datasets, models, transforms\n",
        "<p>device = \"cpu\"\n",
        "<p>def get_model(target_num,isPretrained=False):\n",
        "<p>\n",
        "<p>    model_ft = models.resnet18(pretrained=isPretrained)\n",
        "<p>    model_ft.fc = nn.Linear(512, target_num)\n",
        "<p>    model_ft = model_ft.to(device)\n",
        "<p>    return model_ft\n",
        "<p>※学習のための準備\n",
        "<p>    1. データの検証法選定\n",
        "<p>    2. 最適化手法選定\n",
        "<p>    3. loss関数の選定\n",
        "<p>import torch.optim as optim\n",
        "<p>optimizer = optim.SGD(下記引数)\n",
        "<p>    ・引数params: 最適化するパラメーター\n",
        "<p>    ・引数lr: 学習率（float）\n",
        "<p>    ・引数momentum: モーメンタム（float）\n",
        "<p>    ・引数dampening: モーメンタムの勢い（float）\n",
        "<p>    ・引数weight_decay: L2ノルムの強さ（float）\n",
        "<p>    ・引数nesterov: 乱数の設定\n",
        "<p>optimizer = optim.SGD(model_ft.parameters(),lr=0.001, <p>momentum=0.9)\n",
        "<p>import torch.nn as nn\n",
        "<p>criterion = nn.CrossEntropyLoss()\n",
        "<p>\n",
        "<p>※モデルの学習の流れ\n",
        "<p>※エポック数だけ下記工程の繰り返し\n",
        "<p>    ※ 訓練フェイズ\n",
        "<p>    A. モデルを訓練モードに変更\n",
        "<p>        ※ dataloadersからバッチサイズだけデータ取り出し、下記工程（1−5）の繰り返し\n",
        "<p>        1. optimizerの勾配初期化\n",
        "<p>        2. モデルに入力データをinputし、outputを取り出す\n",
        "<p>        3. outputと正解ラベルから、lossを算出\n",
        "<p>        4. 誤差逆伝播法により勾配の算出\n",
        "<p>        5. optimizerのパラメータ更新\n",
        "<p>    ※ 推論フェイズ        \n",
        "<p>    B. モデルを推論モードに変更\n",
        "<p>        ※ dataloadersからバッチサイズだけデータ取り出し、下記工程（1、２）の繰り返し\n",
        "<p>        1. optimizerの勾配初期化\n",
        "<p>        2. モデルに入力データをinputし、outputを取り出す\n",
        "<p>        3. outputと正解ラベルから、lossを算出\n",
        "<p>    C. 今までのエポックでの精度よりも高い場合はモデルの保存\n",
        "<p>\n",
        "<p>2. モデルの学習\n",
        "<p>train_model（下記引数）\n",
        "<p>    ・引数model: 学習対象のモデル\n",
        "<p>    ・引数criterion: loss関数\n",
        "<p>    ・引数optimizer: 最適化関数\n",
        "<p>    ・引数num_epochs: エポック数（デフォルトは5）\n",
        "<p>    ・引数is_saved: 学習したモデルを保存するか（デフォルトはFalse）\n",
        "<p>※ モデル訓練用関数\n",
        "<p>def train_model(model, criterion, optimizer, num_epochs=5,<p>is_saved = False):\n",
        "<p>    best_acc = 0.0\n",
        "<p>    ※ エポック数だけ下記工程の繰り返し\n",
        "<p>    for epoch in range(num_epochs):\n",
        "<p>        for phase in ['train', 'val']:\n",
        "<p>            print('{}:フェイズ'.format(phase))\n",
        "<p>            ※ 訓練フェイズと検証フェイズの切り替え\n",
        "<p>            if phase == 'train':\n",
        "<p>                model.train() \n",
        "<p>            else:\n",
        "<p>                model.eval()  \n",
        "<p>            running_loss = 0.0\n",
        "<p>            running_corrects = 0\n",
        "<p>            ※ dataloadersからバッチサイズだけデータ取り出し、下記工程<p>（1−5）の繰り返し\n",
        "<p>            for i,(inputs, labels) in enumerate(image_dataloaders[phase]):\n",
        "<p>                inputs = inputs.to(DEVICE)\n",
        "<p>                labels = labels.to(DEVICE)\n",
        "<p>                ※ 1. optimizerの勾配初期化\n",
        "<p>                optimizer.zero_grad()\n",
        "<p>                ※ 2.モデルに入力データをinputし、outputを取り出す\n",
        "<p>                outputs = model(inputs)\n",
        "<p>                _, preds = torch.max(outputs, 1)\n",
        "<p>\n",
        "<p>                ※ 3. outputと正解ラベルから、lossを算出\n",
        "<p>                loss = criterion(outputs, labels)\n",
        "<p>                print('   loaders:{}回目'.format(i+1)  ,'   <p>loss:{}'.format(loss))\n",
        "<p>                if phase == 'train':                        \n",
        "<p>                    ※ 4. 誤差逆伝播法により勾配の算出\n",
        "<p>                    loss.backward()\n",
        "<p>                    ※ 5. optimizerのパラメータ更新\n",
        "<p>                    optimizer.step()\n",
        "<p>                running_loss += loss.item() * inputs.size(0)\n",
        "<p>                running_corrects += torch.sum(preds == labels.data)\n",
        "<p>            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "<p>            epoch_acc = running_corrects.double() / <p>dataset_sizes[phase]\n",
        "<p>            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "<p>            ※ C. 今までのエポックでの精度よりも高い場合はモデルの保存\n",
        "<p>            if phase == 'val' and epoch_acc > best_acc:\n",
        "<p>                best_acc = epoch_acc\n",
        "<p>                if(is_saved):\n",
        "<p>                    torch.save(model.state_dict(), './<p>original_model_{}.pth'.format(epoch))\n",
        "<p>    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "<p>\n",
        "<p>※ エポック数だけ下記工程の繰り返し\n",
        "<p>for epoch in range(num_epochs):\n",
        "<p>for phase in ['train', 'val']:\n",
        "<p>    print('{}:フェイズ'.format(phase))\n",
        "<p>    ※ 訓練フェイズと検証フェイズの切り替え\n",
        "<p>    if phase == 'train':\n",
        "<p>        model.train() \n",
        "<p>    else:\n",
        "<p>        model.eval()\n",
        "<p>※ dataloadersからバッチサイズだけデータ取り出し、下記工程（1−5）の繰り返し\n",
        "<p>for i,(inputs, labels) in enumerate(image_dataloaders[phase]):\n",
        "<p>    inputs = inputs.to(DEVICE)\n",
        "<p>    labels = labels.to(DEVICE)\n",
        "<p>\n",
        "<p>※ 2.モデルに入力データをinputし、outputを取り出す\n",
        "<p>outputs = model(inputs)\n",
        "<p>_, preds = torch.max(outputs, 1)\n",
        "<p>\n",
        "<p>running_loss += loss.item() * inputs.size(0)\n",
        "<p>running_corrects += torch.sum(preds == labels.data)\n",
        "<p>print(torch.sum(preds == labels.data))\n",
        "<p>\n",
        "<p>epoch_loss = running_loss / dataset_sizes[phase]\n",
        "<p>epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "<p>print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "<p>\n",
        "<p>※ C. 今までのエポックでの精度よりも高い場合はモデルの保存\n",
        "<p>if phase == 'val' and epoch_acc > best_acc:\n",
        "<p>    best_acc = epoch_acc\n",
        "<p>    if(is_saved):\n",
        "<p>        torch.save(model.state_dict(), './original_model_{}.<p>pth'.format(epoch))\n",
        "<p>\n",
        "<p>torch.save()\n",
        "<p>\n",
        "<p>3. 学習済みモデルでの推論\n",
        "<p>import pandas as pd\n",
        "<p>df_test = pd.DataFrame(data=os.listdir('./test_data/'))\n",
        "<p>df_test = df_test.rename(columns={0: 'filename'})\n",
        "<p>df_test['target'] = 0\n",
        "<p>df_test.loc[df_test['filename'].str.contains('ok'),'target'] = 1\n",
        "<p>print(df_test['target'].value_counts())\n",
        "<p>\n",
        "<p>推論用データセット作成【Dataset】\n",
        "<p>    1. classとしてDataset作成\n",
        "<p>    2. init関数の定義\n",
        "<p>    3. __len__関数の定義\n",
        "<p>    4. __getitem__関数の定義\n",
        "<p>from torch.utils.data import Dataset\n",
        "<p>class Test_Datasets(Dataset):\n",
        "<p>    def __init__(self, data_transform):\n",
        "<p>        処理XXXX,\n",
        "<p>        処理XXXX,\n",
        "<p>    def __len__(self):\n",
        "<p>        処理XXXX,\n",
        "<p>        処理XXXX,\n",
        "<p>    def __getitem__(self, index):\n",
        "<p>        処理XXXX,\n",
        "<p>        処理XXXX,\n",
        "<p>        return\n",
        "<p>※ __XX__は特殊メソッド（Special method）\n",
        "<p>\n",
        "<p>import pandas as pd\n",
        "<p>from torch.utils.data import Dataset\n",
        "<p>class Test_Datasets(Dataset):\n",
        "<p>    def __init__(self, data_transform):\n",
        "<p>        self.df = pd.read_csv('./df_test.csv',names=['filename','target'])\n",
        "<p>        self.data_transform = data_transform\n",
        "<p>    def __len__(self):\n",
        "<p>    def __getitem__(self, index):\n",
        "<p>        return\n",
        "<p>import pandas as pd\n",
        "<p>from torch.utils.data import Dataset\n",
        "<p>from PIL import Image\n",
        "<p>\n",
        "<p>class Test_Datasets(Dataset):\n",
        "<p>    def __init__(self, data_transform):\n",
        "<p>        self.df = pd.read_csv('./df_test.csv',names=['filename','target'])\n",
        "<p>        self.data_transform = data_transform\n",
        "<p>    def __len__(self):\n",
        "<p>        return len(self.df)\n",
        "<p>    def __getitem__(self, index):\n",
        "<p>        file = self.df['filename'][index]\n",
        "<p>        image = Image.open('./test_data/'+ file)\n",
        "<p>        image = self.data_transform(image)\n",
        "<p>        return image,file\n",
        "<p>\n",
        "<p>torch.utils.data.DataLoader(下記引数)\n",
        "<p>    ・引数dataset: loaderで読み込むデータセット\n",
        "<p>    ・引数batch_size: バッチサイズ\n",
        "<p>    ・引数shuffle: データをshuffleするか（True もしくは False）\n",
        "<p>    ・引数num_workers: 実行プロセス数（デフォルトは0）\n",
        "<p>    ・引数drop_last: 残りデータ数がバッチサイズに満たない場合にスキップするか（True もしくは False）\n",
        "<p>\n",
        "<p>import torch.nn as nn\n",
        "<p>import torch\n",
        "<p>from torchvision import datasets, models, transforms\n",
        "<p>DEVICE= \"cpu\"\n",
        "<p>def get_model(target_num,isPretrained=False):\n",
        "<p>    model_ft = models.resnet18(pretrained=isPretrained)\n",
        "<p>    model_ft.fc = nn.Linear(512, target_num)\n",
        "<p>    model_ft = model_ft.to(DEVICE)\n",
        "<p>    return model_ft\n",
        "<p>best_model = get_model(target_num=2)\n",
        "<p>\n",
        "<p>torch.nn.Module.load_state_dict()\n",
        "<p>torch.load()\n",
        "<p>best_model.load_state_dict(torch.load('./original_model_33.<p>pth', map_location=lambda storage, loc: storage), strict=True)\n",
        "<p>pred = []\n",
        "<p>\n",
        "<p>※ データの取り出し\n",
        "<p>for i,(inputs, labels) in enumerate(test_dataloader):\n",
        "<p>    inputs = inputs.to(DEVICE)\n",
        "<p>    ※ 学習済みモデルを推論モードに設定\n",
        "<p>    best_model.eval()\n",
        "<p>    ※ 学習済みモデルにデータをインプットし、推論をさせる\n",
        "<p>    outputs = best_model(inputs)\n",
        "<p>    ※ アウトプットから推定されたラベルを取得\n",
        "<p>    _, preds = torch.max(outputs, 1)\n",
        "<p>    ※ 事前に用意したリストに推論結果(0 or 1)を格納\n",
        "<p>    pred.append(preds.item())\n",
        "<p>df_test['pred'] = pred\n",
        "<p>\n",
        "<p>\n",
        "<p>---\n",
        "<p>\n",
        "<p>\n",
        "<p>④欠陥検出モデルの作成【転移学習有】\n",
        "<p>1. モデル作成/学習【転移学習有】\n",
        "<p>※ ライブラリのインポート\n",
        "<p>import torch\n",
        "<p>import torch.nn as nn\n",
        "<p>from torchvision import datasets, models, transforms\n",
        "<p>※ 定数設定\n",
        "<p>device = \"cpu\"\n",
        "<p>TARGET_NUM = 2\n",
        "<p>※ モデル作成関数の定義\n",
        "<p>def get_model(target_num,isPretrained=False):\n",
        "<p>    model_ft = models.resnet18(pretrained=isPretrained)\n",
        "<p>    model_ft.fc = nn.Linear(512, target_num)\n",
        "<p>    model_ft = model_ft.to(device)\n",
        "<p>    return model_ft\n",
        "<p>\n",
        "<p>pretrained_model = get_model(target_num=TARGET_NUM,<p>isPretrained=True)\n",
        "<p>\n",
        "<p>torch.nn.Module.load_state_dict()\n",
        "<p>\n",
        "<p>2. モデルの精度改善\n",
        "<p>import torch\n",
        "<p>import matplotlib.pyplot as plt\n",
        "<p>from PIL import Image\n",
        "<p>from torchvision import transforms, datasets\n",
        "<p>def show_augment(dataset):\n",
        "<p>    for i in range(0, 6):\n",
        "<p>        ax = plt.subplot(2, 3, i + 1)\n",
        "<p>        plt.tight_layout()\n",
        "<p>        ax.set_title(str(i))\n",
        "<p>        plt.imshow(dataset[i][0])\n",
        "<p>    plt.show()\n",
        "<p>\n",
        "<p>noaug_dataset = datasets.ImageFolder(root='./augment_data/')\n",
        "<p>        show_augment(noaug_dataset)\n",
        "<p>\n",
        "<p>transform = transforms.Compose([\n",
        "<p>                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "<p>                       ])\n",
        "<p>aug_dataset = datasets.ImageFolder(root='./augment_data/',<p>transform=data_transform)\n",
        "<p>        show_augment(aug_dataset)\n",
        "<p>\n",
        "<p>import zipfile\n",
        "<p>import torch\n",
        "<p>import matplotlib.pyplot as plt\n",
        "<p>import numpy as np\n",
        "<p>from PIL import Image\n",
        "<p>from torchvision import transforms, datasets\n",
        "<p>def unzip_dataset(INPATH,OUTPATH):\n",
        "<p>    with zipfile.ZipFile(INPATH) as zf:\n",
        "<p>        zf.extractall(OUTPATH)\n",
        "<p>unzip_dataset(INPATH='./augment_data.zip',OUTPATH='./')\n",
        "<p>def show_augment(dataset):\n",
        "<p>    for i in range(0, 6):\n",
        "<p>        ax = plt.subplot(2, 3, i + 1)\n",
        "<p>        plt.tight_layout()\n",
        "<p>        ax.set_title(str(i))\n",
        "<p>        plt.imshow(dataset[i][0])\n",
        "<p>    plt.show()\n",
        "<p>\n",
        "<p>\n",
        "<p>import albumentations\n",
        "<p>albu_transforms = albumentations.Compose([\n",
        "<p>                          albumentations.RandomGridShuffle(grid=(3, 3), p=1.0), \n",
        "<p>                        ])\n",
        "<p>def albumentations_transform(image, transform=albu_transforms):\n",
        "<p>            image_np = np.array(image)\n",
        "<p>            augmented = transform(image=image_np)\n",
        "<p>            image = Image.fromarray(augmented['image'])\n",
        "<p>            return image\n",
        "<p>\n",
        "<p>\n",
        "<p>data_transform = transforms.Compose([\n",
        "<p>          transforms.Lambda(albumentations_transform),\n",
        "<p>        ])\n",
        "<p>dataset_augmentated = datasets.ImageFolder(root='./<p>augment_data/', transform=data_transform)\n",
        "<p>show_augment(dataset_augmentated)\n",
        "<p>\n",
        "<p>\n",
        "<p>albumentations.Cutout(下記引数)\n",
        "<p>    ・引数num_holes: ドロップする領域の最大数\n",
        "<p>    ・引数max_h_size: ドロップする領域の縦幅最大\n",
        "<p>    ・引数max_w_size: ドロップする領域の横幅最大\n",
        "<p>    ・引数fill_value: ドロップされた画素の値\n",
        "<p>    ・引数p: 発生確率\n",
        "<p>\n",
        "<p>\n",
        "<p>import albumentations\n",
        "<p>albu_transforms = albumentations.Compose([\n",
        "<p>                          albumentations.Cutout(num_holes=8, max_h_size=20, max_w_size=20, fill_value=0, p=1.0),\n",
        "<p>                        ])\n",
        "<p>def albumentations_transform(image, transform=albu_transforms):\n",
        "<p>            image_np = np.array(image)\n",
        "<p>            augmented = transform(image=image_np)\n",
        "<p>            image = Image.fromarray(augmented['image'])\n",
        "<p>           return image\n",
        "<p>data_transform = transforms.Compose([\n",
        "<p>          transforms.Lambda(albumentations_transform),\n",
        "<p>        ])\n",
        "<p>dataset_augmentated = datasets.ImageFolder(root='./<p>augment_data/', transform=data_transform)\n",
        "<p>show_augment(dataset_augmentated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcOjuVDowq8"
      },
      "source": [
        "## 学習の流れ\n",
        "<p>モデル：ResNet18\n",
        "<p>転移学習：無\n",
        "<p>データ量\n",
        "<p>学習データ：6633枚\n",
        "<p>検証データ：715枚\n",
        "<p>optimizer：SGD(lr=0.001, momentum=0.9)\n",
        "<p>loss関数：CrossEntropyLoss\n",
        "<p>バッチサイズ:32\n",
        "<p>エポック：100\n",
        "<p>データ前処理\n",
        "<p>リサイズ（256）\n",
        "<p>ランダム水平反転（p=0.5）\n",
        "<p>テンソル化\n",
        "<p>正規化\n",
        "<p>*【学習時loss】*"
      ]
    }
  ]
}